\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}          
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{csquotes}
\usepackage{setspace}\onehalfspacing
\usepackage[loose,nice]{units} %replace "nice" by "ugly" for units in upright fractions
\usepackage{graphicx}
\usepackage{listings}
\usepackage{color}
\usepackage{glossaries}	
\usepackage{enumitem}% http://ctan.org/pkg/enumitem
\usepackage[center, md]{titlesec} % add ", md" behind center to remove bolding


\graphicspath{ {/Users/eghuang/math104/images/} }

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=R,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\title{
  Algorithmic information and inference devices }
\author{Edward G. Huang \\ {\small Santa Fe Institute } \\ {\small Department of Mathematics, University of California, Berkeley } \\ \lstinline{eghuang@berkeley.edu}}
\date{ {\normalsize August 2018} }

 \newcommand{\R}{\mathbb{R}}
 \newcommand{\Q}{\mathbb{Q}}
 \newcommand{\Z}{\mathbb{Z}}
 \newcommand{\N}{\mathbb{N}}
 \newcommand{\B}{\mathbb{B}}
 \newcommand{\Prob}{\mathbb{P}}
 \newcommand{\E}{\mathbb{E}}
 \newcommand{\code}[1]{\texttt{#1}}
 \let\oldemptyset\emptyset
 \let\emptyset\varnothing
 \setlength{\parindent}{0pt} % no indent
 
\begin{document}
\maketitle 
% HOOPER: Please include an abstract, a brief intro with a clear statement of your research question(s), a description of your methods/model/approach, your results, and your conclusions (or lessons learned).
% SECTION 0: ABSTRACT
\renewcommand{\abstractname}{\vspace{-\baselineskip}}
\begin{abstract}
There has been much interest surrounding what properties about the universe can be derived from applying a mathematical formalization of inference and knowledge. Previous work by Wolpert used the theory of "inference devices" (IDs) to demonstrate bounds on knowledge in any physical universe that allows agents to hold information concerning that universe. We extend previous work on the capacity and limitations of IDs to infer physical variables. Our results impose conditions on the inference of singular functions and sets of functions. We pursue analogues between IDs and their relation to Turing machine theory and algorithmic information theory. In particular, we show that any Turing machine can be strongly inferred and build upon that to demonstrate incompressibility of strong inference complexity. This incompressibility result has led to several analogues between Kolmogorov complexity and inference complexity that suggest further similarities between algorithmic information theory and the theory of inference devices.
\end{abstract}

% SECTION 1: INTRODUCTION
\section{Introduction} 
Observe that information concerning the universe is typically held by some agent embedded within that universe. Agents typically acquire information concerning the universe in at least four ways: prediction, observation, control, and memory. As discussed by Wolpert, there is a mathematical structure shared by all such information-acquiring avenues. Objects with that structure are called inference devices [1].  \\

This work explores the relationship between inference devices, Turing machines, and Kolmogorov complexity. We introduce the mathematical structures of inference devices, weak inference, and strong inference and examine the conditions in which inference holds. Basic forms of Turing machines and their relationship to computability and partial recursive functions are reviewed. We then discuss the inference complexity and possible connections to Kolmogorov complexity. Finally we touch upon physical knowledge, a mathematical formalism of knowledge in a physical universe. Several properties of physical knowledge are extended and explored. These properties suggest that the mathematical definition of physical knowledge may require further examination and modification. \\

% SUBSECTION: NOTATION
\subsection{Notation and Definitions}
This manuscript utilizes standard notation taken from set theory and vector algebra. We clarify notation specific to Turing machine theory and inference devices. 

\begin{center}
\subsubsection{Turing Machine Notation} 
\begin{tabular}{cp{0.8\textwidth}}
$ \B^{*} $ & The space of all finite bit strings, $ \{\epsilon, 0, 1, 00, 01, \dots \}. $\\
$ \Lambda $ & Turing machine symbol alphabet. \\
$ \sigma  $ & Symbol on a Turing machine tape. \\
$ Q $ & Set of finite Turing machine states. \\
$ \Delta $ & Turing machine transition function. \\
$ k $ & Number of tapes of a Turing machine. The first tape is assumed to be read-only. \\
$ \eta $ & Non-halting state of a Turing machine. \\
$ \phi $ & The universal partial recursive function. \\
$ (\Lambda, Q, \Delta) $ & A Turing machine. 
\end{tabular}

\subsubsection{Inference Device Notation} 
\begin{tabular}{cp{0.8\textwidth}}
$ U $ & Set of possible histories of the universe. \\
$ u $ & A history of the universe in $ U $. \\ 
$ X $ & Setup function of an ID that maps $ U \rightarrow X(U) $. A binary question concerning $ \Gamma(u) $. \\
$ x $ & A binary question and a member of image $ X(U) $. \\ 
$ Y $ & Single-valued conclusion function of an ID that maps $ U \rightarrow \{-1, 1\} $. A binary answer of an ID for $ X(u) = x $. \\ 
$ y $ & A single-valued answer, and member of image $ Y(U)  = \{0, 1\} $. \\ 
$ \Gamma $ & A function of the actual values of a physical variable over $U$.  \\
$ \gamma $ & Possible value of a physical variable, a member of the image $\Gamma(U)$. \\
$ \delta $ & Probe of any variable $V$ parameterized by $v \in V$ such that : 
	  \[ \delta_v (v') =
	  \begin{cases} 
       1 & \text{ if } v = v' \\
       -1 & \text{ otherwise } \\
      \end{cases}\] \\
$ \wp $ & Set of probes over $\Gamma(U)$. \\
$ \mathcal{D}  $ & An inference device, consisting of functions $ (X, Y) $. \\
$ \xi $ & A function $ \Gamma(U) \rightarrow \overline{X} $ used for knowing $ \gamma $ over $ W \subseteq U $.  \\
$ \Gamma^{-1} $ & Inverse. Given a function $ \Gamma $ over $ U $, $\Gamma ^ {-1} = \bar{\Gamma} \equiv \{\{u : \Gamma(u) = \gamma \} : \gamma \in \Gamma(U) \} $. \\
$ \overline{\Gamma} $ & Given a function $ \Gamma $ over $ U $, the partition of $ U $ given by $ \Gamma^{-1} $.  \\
$ > $ & Weakly infers.  \\
$ \gg $ & Strongly infers. \\
$ \mathcal{C}_{\mu}(\Gamma; \mathcal{D}) $ & Inference complexity. \\
$ \mathcal{K}_{\mu}(\gamma; \mathcal{D}) $ & Knowledge complexity. \\
$ \mu $ & Measure defined for $ u \in U $. \\
$ \ell $ & Length of a bit string. \\
\end{tabular}
\end{center}

% SUBSECTION: INFERENCE DEVICE
\subsection{Inference Devices}
We briefly consider the structure of inference devices. Let $ U $ be the set of all possible histories. Define $ \Gamma $ as a function over $ U $ such that $ \gamma \in \Gamma $ represents some the value of some physical variable over $ U $. Then the binary question $ x $ that an agent asks themselves concerning $ \gamma \in \Gamma $ can be generated by a function $ X $ over $ U $. Similarly, let $ Y $ be the function over $ U $ that provides a binary answer to $ x $. Then an inference device can be defined. \\

\textsc{Definition} \; An \textbf{(inference) device} over a set $ U $ is a pair of functions $ (X, Y) $, both with domain $ U $. $ Y $ is called the \textbf{conclusion} function of the device, and is surjective onto $ \B $. $ X $ is called the \textbf{setup} function of the device. \\

Additional structure is necessary to formalize the notion of knowledge held by an inference device in a universe. We next discuss structures that capture some notion when a device can correctly infer something in its universe.

\subsubsection{Types of Inference}

A device must be able to answer some questions correctly to infer knowledge concerning a physical variable. We can encapsulate this concept in the definition of weak inference. \\

\textsc{Definition} \; A device $\mathcal{D}$ \textbf{weakly infers} $\Gamma$ \textit{iff} $ \forall \gamma \in \Gamma(U), \exists x \in X(U) $ s.t. $ \forall u \in U $, 
$ X(u) = x \implies Y(u) = \delta_{\gamma}(\Gamma(ru)) $. \\

\textsc{Remark} \; A device does not need to understand a question to satisfy weak inference. In other words, a device can weakly infer $ \Gamma $ even if it correctly answers questions accidentally. \\

We wish to strengthen the notion of weak inference such that a device can emulate the entire mapping of a function. Let $ S $ be a function over $ U $ at represents the possible inputs for a function $ f $. Continue to define $ T $ over $ U $ as the output of $ f $ given $ s $. Then we can write $ f(s) = T(S^{-1}(s)) $ as the function a device wishes to emulate. Now we can define the notion of strong inference for the function $ f $. \\

\textsc{Definition} \; A device $ \mathcal{D} $ \textbf{strongly infers} a function $ (S, T) $ over $ U $ \textit{ iff } $\forall \delta \in \wp(T) $ and all $ s \in S(U) $, $ \exists x $ \textit{ such that } $ X(u) = x \implies S(u) = s, Y(u) = \delta(T(u)) $. \\


% SUBSECTION: TURING MACHINES
\subsection{Turing Machines} 
A Turing machine is a mathematical model of computation that describes an abstract machine that manipulates symbols on a tape according to a set of rules. For every computer algorithm, there is a Turing machine that can simulate the behavior of that algorithm. By the Church-Turing Thesis, any algorithm over the natural numbers is computable by a human if and only if the algorithm is computable by a Turing machine. We review the formal construction of a Turing machine and two variants.

% SUBSECTION: DTM
\subsubsection{Deterministic Turing Machines} 
 Arora and Barak [2] denote a Turing Machine (TM) as $ T = (\Lambda, Q, \Delta) $ containing:
\begin{enumerate}
\item An \textit{alphabet} $ \Lambda $ of a finite set of symbols that $ T $'s tapes can contain. We assume that $ \Lambda $ contains a special blank symbol $ B $, start symbol $ S $, and the symbols 0 and 1. 
\item A finite set $ Q $ of possible states that $ T $'s register can be in. We assume that $ Q $ contains a special start state $ q_{s} $ and a special halt state $ q_{h} $. 
\item A transition function $ \Delta : Q \times \Lambda^{k} \rightarrow Q \times \Lambda^{k - 1} \times \{L, \mathcal{S}, R\}^{k} $, where $ k \geq 2$, describing the rules $ T $ use in performing each step. The set $\{L, \mathcal{S}, R\}$ denote the actions \textit{Left, Stay,} and \textit{Right}, respectively. 
\end{enumerate}

Suppose $ T $ is in state $ q \in Q $ and $ (\sigma_1, \sigma_2, \dots, \sigma_k) $ are the symbols on the $ k $ tapes. Then $ \Delta(q, (\sigma_1, \dots, \sigma_k)) = (q', (\sigma_{2}^{'}, \dots, \sigma_{k}^{'}), z) $ where $ z \in \{L, \mathcal{S}, R\}^k $ and at the next step the $ \sigma $ symbols in the last $ k - 1 $ tapes will be replaced by the $ \sigma' $ symbols, the machine will be in state $ q $, and the $ k $ heads will move \textit{Left, Right} or \textit{Stay}. This is illustrated in the following figure. \\

\textsc{Figure.} \; The transition function $ \Delta $ for a $ k $-tape Turing Machine

 \begin{center}
 \begin{tabular}{ c|c|c|c||c|c|c|c|c } 
 \hline
 \multicolumn{4}{c||}{ $ (q, (\sigma_1, \dots, \sigma_k)) $ } & 
      \multicolumn{5}{c}{ $ (q', (\sigma_{2}^{'}, \dots, \sigma_{k}^{'}), z) $ } \\
 
 \hline
 \begin{tabular}[c]{@{}c@{}} Input \\ symbol \end{tabular} & 
 \begin{tabular}[c]{@{}c@{}} Work/output \\ symbol \\ read \end{tabular} & 
 $\dots $ &
 \begin{tabular}[c]{@{}c@{}} Current \\ state \end{tabular} &
 \begin{tabular}[c]{@{}c@{}} New \\ work/output \\ tape symbol \end{tabular} & 
 $ \dots $ &
 \begin{tabular}[c]{@{}c@{}} Move \\ work/output \\ tape \end{tabular} &
 $ \dots $ &
 \begin{tabular}[c]{@{}c@{}} New \\ state \end{tabular} \\ 
 
 \hline
 \hline
 $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ \\ 
 \hline
 $  \sigma_1 $ & $ \sigma_i $ & $ \ddots $ & $ q $ & $ \sigma_i^{'} $ & $ \ddots $ & $ z_i $ & $ \ddots $ & $ q^{'} $ \\ 
 \hline
 $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ \\ 
 \end{tabular}
 \end{center} 

\bigbreak

\textsc{Remark.} $\Lambda$ can be reduced to $ \B = \{0, 1\} $ and $ k $ can be reduced to $ 1 $ without loss of computational power. Then, any Turing Machine can be expressed as a partial recursive function mapping $ \B^{*} \rightarrow \B^{*} \cup \eta $, where $ \eta $ is the undefined non-halting output. Since $ |\B^{*} \times \B^{*} \cup \eta | = | \N \times \N | = | \N | $, the set of all Turing machines is countably infinite. \\

% SUBSECTION: NDTM
\subsubsection{Non-deterministic Turing Machines} 
Non-deterministic Turing Machines (NDTM) differ from deterministic Turing Machines by having two transition functions $ \Delta_0, \Delta_1 $ and a special state $ q_{accept} $. From Arora and Barak [2]:

\begin{displayquote}
When a NDTM $M$ computes a function, we envision that at each computational step $ M $ makes an arbitrary choice as to which of its two transition functions to apply. For every input $ x $, we say that $ M(x) = 1 $ if there exists some sequence of these choices (which we call nondeterministic choices of $ M $) that would make $ M $ reach $ q_{accept} $ on input $ x $. Otherwise - if every sequence of choices makes $ M $ halt without reaching $ q_{accept} $ - then we say that $ M(x) = 0 $. 
\end{displayquote}

If $ M(x) = 1$, we say that $ M $ accepts the input $ x $. There are two ways to interpret the choice of update function to use in a NDTM. We can either assume that the NDTM chooses updates that will lead to an accepting state, or we can assume that the machine branches out into its choices such that it has a "computation tree" and if any of the branches reaches the accepting state then the machine accepts the input. From this second interpretation, the computational power of DTMs to NDTMs is analogous to the computational complexity of P to NP. 

% SUBSECTION: UTM
\subsubsection{Universal Turing Machines}
There exists a universal Turing machine that can simulate the execution of any other Turing machine $ M $ given the description of $ M $ as an input. It is a fundamental result that these machines can be explicitly constructed and that there are an infinite number of such. To imitate the behavior of $ M $, a universal machine simulates the actions of $ M $ on a representation of the tape contents of $ M $. The partial recursive function computed by any universal machine is called the universal partial recursive function.

% SECTION: WEAK INFERENCE
\section{Weak Inference}
\bigskip   

\textsc{Definition} \quad Two functions $ \Gamma_1 $ and $ \Gamma_2 $ with the same domain $ U $ are \textbf{(functionally) equivalent} iff the inverse functions $ \Gamma^{-1}_1 $ and $ \Gamma^{-1}_2 $ induce the same partitions of $ U $, i.e., iff $ \overline{\Gamma_1} = \overline{\Gamma_2} $. \\


The condition in this definition is equivalent to requiring the preimages of the two functions to be inequivalent. If two functions are equivalent and a device weakly infers one of the functions, then the device also infers the other. Now we address some conditions on functions that can be weakly inferred. \\
% LEMMA 1

\textsc{Lemma} \; \textit{A function} $ \Gamma $ \textit{can be weakly inferred by a device} $ \mathcal{D} $ \textit{if} $ |\Gamma^{-1}(\gamma)| \geq 2 $ \textit{for any} $ \gamma \in \Gamma(U) $. \\
\textsc{Proof} \; Let $ U := \N $. Enumerate $ \gamma \in \Gamma $ as $ 1, 2, \dots, i, \dots, n $ and define $ V^{i} = \{u: \Gamma^{-1}(i) = u \} $. Then enumerate each element in $ V^i $ as $ i_1, i_2, \dots, i_j, \dots, i_m $. Continue to define $ X $ and $ Y $ as 

\begin{equation*}
X(i_j) = \begin{cases}
       a_i & \text{ if } j = 2 \\
       b_i & \text{ otherwise } \\
       \end{cases} \quad \quad 
Y(i_j) = \begin{cases}
       -1 & \text{ if } j = 2 \\
       1 & \text{ otherwise. } \\
       \end{cases} 
\end{equation*}

Then for each $ i \in \Gamma(U) $ choose $ x = b_i $ to force $ Y(X^{-1}(b_i)) = \delta_{i}(\Gamma(X^{-1}(b_i)) = i) = 1. \hfill \qed $ \\

% LEMMA 2
\bigskip   
\textsc{Lemma} \; \textit{A function} $ \Gamma $ \textit{can be weakly inferred by a device} $ \mathcal{D} $ \textit{if} $ |\Gamma(U)| \geq 3 $.  \\
\textsc{Proof} \; Let $ U := \N $. Enumerate $ \gamma \in \Gamma(U) $ as $ 1, 2, \dots, i, \dots, n $ and define $ V^{i} = \{u: \Gamma^{-1}(i) = u \} $. Then enumerate each element in $ V^i $ as $ i_1, i_2, \dots, i_j, \dots, i_m $. Continue to define $ X $ and $ Y $ as 

\begin{equation*}
X(i_j) = \begin{cases}
       a & \text{ if } i = 1 \text{ and } j = 1 \\
       b & \text{ if } i = 2 \text{ and } j = 1 \\
       c & \text{ otherwise. } \\
       \end{cases} \quad \quad 
Y(i_j) = \begin{cases}
       1 & \text{ if } i = 1 \text{ and } j = 1 \\
       -1 & \text{ otherwise. } \\
       \end{cases} 
\end{equation*}

Then for each $ i \in \Gamma(U) $ to force $ Y(X^{-1}(x)) = \delta_{i}(\Gamma(X^{-1}(x)) = i) $ choose $ x = a $ if $ i = 1 $, $ x = c $ if $ i = 2 $, or else choose $ x = b $. $ \hfill \qed $ \\

% COROLLARY 2
\bigskip
\textsc{Corollary} \; \textit{A function $\Gamma$ cannot be weakly inferred by ay device if $ |\Gamma^{-1}(\gamma)| <  2 $ and $ |\Gamma(U)| < 3 $}. \\
\textsc{Proof} \; Fix any such $ \Gamma $ and set $ U = \{1, 2\} $. Let $ \Gamma(1) = \gamma_1 $, $\Gamma(2) = \gamma_2 $. Then either $ Y(1) = 1, Y(2) = -1 $, or vice versa. For the first case, consider the probe $ \delta_{\gamma_{2}} $ to see that there is no $ x \in X(U) $ that forces $ Y(u) = \delta_{\gamma_{2}}(\Gamma(u)) $. Consider $ \delta_{\gamma_{1}} $ to see that weak inference does not hold for the remaining case.  \hfill \qed \\

These results are enough to fully determine whether a single function is weakly inferable. We continue to examine the inference of sets of functions by a singular device. 

% COUNTABLE INFERENCE THEOREM
\bigskip
\textsc{Theorem} \; \textit{A countable set of inequivalent functions} $ A^{*} $ \textit{can be weakly inferred by a device if each function $ A_i \in A^{*} $ is independently inferable. } \\

\textsc{Proof} \; Let $ U := \N $ and fix any $ A^{*} $. Let $ a_i, a_j, a_k $ represent any distinct three elements in $ A_i(U) $. Write $ V = \{1, 2, 3\} \subset U $. The following table represents all possible combinations of $ a_i $, $ a_j$, and $ a_k $ over $ V $. \\
 \begin{center}
 \begin{tabular}{ |c||c|c|c|c|c|c|c|c } 

 \hline
 $ u $ & $ X(u) $ & $Y(u)$ & $ A_1(u) $ & $ A_2(u) $ & $ A_3(u) $ & $ A_4(u) $ & $ A_5(u) $ & $ \dots $ \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ -1 $ & $ a_i $ & $ a_i $ & $ a_i $ & $ a_i $ & $ a_i $ & $ \dots $ \\
 \hline
 $ 2 $ & $ 2 $ & $ -1 $ & $ a_i $ & $ a_j $ & $ a_j $ & $ a_j $ & $ a_i $ & $ \dots $ \\
 \hline
 $ 3 $ & $ 3 $ & $ 1 $ & $ a_i $ & $ a_k $ & $ a_j $ & $ a_i $ & $ a_j $ & $ \dots $ \\
 \hline
 $ \vdots $ & $ \vdots $ & $ \vdots $ & $ \vdots $ & $ \vdots $ & $ \vdots $ & $ \vdots $ & $ \vdots $ & $ \ddots $ \\
 \end{tabular} 
 \end{center}
\bigskip 
Note that setting $ X(u) = u $, $ Y(1) = -1 $, $ Y(2) = -1 $, and $ Y(3) = 1 $ immediately satisfies weak inference for any function $ A_i $ that is functionally equivalent to $ A_1 $, $ A_2 $, $ A_3 $ and $ A_4 $ over $ V $. This holds regardless of the values of $ Y(u) $ and $ A_{ i \in \{1, 2, 3, 4\} }(u) $ that follow for $ u > 3 $. This is shown by selecting $ x $ for the following cases:

\bigskip
\textit{Case 1:} $ \overline{A_i(V)} = \overline{A_1(V)} $. \\
 Choose $ x = 3 $ for $ a = a_i $ or otherwise choose $ x = 2 $. \\

\textit{Cases 2, 3, 4:} $ \overline{A_i(V)} = \overline{A_2(V)} $ or  $ \overline{A_3(V)} $ or $ \overline{A_4(V)} $. \\
 Choose $ x = 2 $ for $ a = a_i $ or otherwise choose $ x = 1 $. \\

Now we need to guarantee weak inference for $ A_i $ that are functionally equivalent to $ A_5 $ over $ V $. Enumerate each $ A_i \in A^{*}: \overline{A_i(V)} = \overline{A_5(V)} $ as $ B_4, B_5, \dots, B_i, \dots, B_n $. To satisfy weak inference for any $ A_i $ and $ B_i $, define $ X $, $ Y $ more explicitly as 
\begin{equation*} 
     X(u) = u, \quad \quad
     Y(u) = \begin{cases} 
    -1 & \text{ if } u = 1, 2 \text { or if } B_i(u) = a_j, a_k: X(u) = i \\
     1 & \text{ if } u = 3 \text{ or if } B_i(u) = a_i: X(u) = i \\
    -1 & \text { otherwise. } \\
            \end{cases} \\
\end{equation*}

For each $ a \in B_i(U) $ to force $ Y(X^{-1}(x)) = \delta_{a}(B(X^{-1}(x))) $ choose $ x = i $ if $ a = a_i $ or otherwise choose $ x = 1 $. \hfill \qed  \\

This result is an example of the weakness of the definition of weak inference. Note that a device only needs two values of $ u $ to infer any single function $ \Gamma $, and that for a set, these $ u $ do not always need to be unique for each $ \Gamma $. In fact, a device never needs to answer affirmatively that $ \gamma = \Gamma(U) $. This flexibility allows for the weak inference of a countably infinite inequivalent set of functions using only two $ u $. These properties of weak inference are crucial to later investigations concerning the magnitude of information required to infer sets of functions.


% SECTION 3: STRONG INFERENCE
\section{Strong Inference} 
 
In the next three examples we examine strong inference of integer-valued functions. The right table indicates $ x $ for each $ s $, $ \delta $ such that the definition of strong inference is satisfied. \\
 
% EXAMPLE 
 \textsc{Example.} \quad Let $ T(U) = \{0 , 1\} $ and $ S(U) = \{0, 1, 2\} $. We construct $ (X, Y) $ in the table at the left such that it strongly infers $ (S, T) $. \\ 
 \begin {center}
 \begin{tabular}{ |c||c|c|c|c| } 

 \hline
 $ u $ & $ X(u) $ & $ Y(u) $ & $ S(u) $ & $ T(u) $ \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ 1 $ & $ 0 $ & $ 0 $ \\
 \hline
 $ 2 $ & $ 2 $ & $ -1 $ & $ 0 $ & $ 0 $ \\
 \hline
 $ 3 $ & $ 3 $ & $ 1 $ & $ 1 $ & $ 0 $ \\
 \hline
 $ 4 $ & $ 4 $ & $ -1 $ & $ 1 $ & $ 0 $ \\
 \hline 
 $ 5 $ & $ 5 $ & $ 1 $ & $ 2 $ & $ 1 $ \\
 \hline 
 $ 6 $ & $ 6 $ & $ -1 $ & $ 2 $ & $ 1 $ \\
 \hline 
 \end{tabular} 
 \quad  
 \begin{tabular}{ |c||c|c| } 

 \hline
 $ s $ \textbackslash $ \delta $ & $ \delta_0 $ & $ \delta_1 $ \\ 
 \hline
 \hline
 $ 0 $ & $ 1 $ & $ 2 $  \\
 \hline
 $ 1 $ & $ 3 $ & $ 4 $ \\
 \hline
 $ 2 $ & $ 6 $ & $ 5 $ \\
 \hline
 
 \end{tabular}
 \end{center}
 $ \hfill \diamond $ 
 
% EXAMPLE  
 \bigskip 
 \textsc{Example.} \quad Let $ T(U) = \{1, 2, 3\} $ and $ S(U) = \{1, 2, 3, 4, 5\} $. \\ 

 \begin{center}
 \begin{tabular}{ |c||c|c|c|c| } 

 \hline
 $ u $ & $ X(u) $ & $ Y(u) $ & $ S(u) $ & $ T(u) $ \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ 1 $ & $ 1 $ & $ 1 $ \\
 \hline
 $ 2 $ & $ 2 $ & $ -1 $ & $ 1 $ & $ 1 $ \\
 \hline
 $ 3 $ & $ 3 $ & $ 1 $ & $ 2 $ & $ 1 $ \\
 \hline
 $ 4 $ & $ 4 $ & $ -1 $ & $ 2 $ & $ 1 $ \\
 \hline
 $ 5 $ & $ 5 $ & $ 1 $ & $ 3 $ & $ 2 $ \\
 \hline
 $ 6 $ & $ 6 $ & $ -1 $ & $ 3 $ & $ 2 $ \\
 \hline
 $ 7 $ & $ 7 $ & $ 1 $ & $ 4 $ & $ 2 $ \\
 \hline
 $ 8 $ & $ 8 $ & $ -1 $ & $ 4 $ & $ 2 $ \\
 \hline
 $ 9 $ & $ 9 $ & $ 1 $ & $ 5 $ & $ 3 $ \\
 \hline
 $ 10 $ & $ 10 $ & $ -1 $ & $ 5 $ & $ 3 $ \\
 \hline 
 \end{tabular} 
 \quad
 \begin{tabular}{ |c||c|c|c| } 

 \hline
 $ s $ \textbackslash $ \delta $ & $ \delta_1 $ & $ \delta_2 $ & $ \delta_3 $  \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ 2 $ & $ 2 $ \\
 \hline
 $ 2 $ & $ 3 $ & $ 4 $ & $ 4 $ \\
 \hline
 $ 3 $ & $ 6 $ & $ 5 $ & $ 6 $ \\
 \hline
 $ 4 $ & $ 8 $ & $ 7 $ & $ 8 $ \\
 \hline 
 $ 5 $ & $ 10 $ & $ 10 $ & $ 9 $ \\
 \hline
 
 \end{tabular}
 \end{center}
 $ \hfill \diamond $ 

% EXAMPLE
 \bigskip 
 \textsc{Example.} \quad Let $ T(U) = \{1, 2, 3\} $ and $ S(U) = \{1, 2\} $. In this example, the inferred function $ f: S \rightarrow T, f(s) = T(S^{-1}(s)) $ is not single-valued. \\ 

 \begin{center}
 \begin{tabular}{ |c||c|c|c|c| } 

 \hline
 $ u $ & $ X(u) $ & $ Y(u) $ & $ S(u) $ & $ T(u) $ \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ -1 $ & $ 1 $ & $ 1 $ \\
 \hline
 $ 2 $ & $ 2 $ & $ -1 $ & $ 1 $ & $ 2 $ \\
 \hline
 $ 3 $ & $ 3 $ & $ 1 $ & $ 1 $ & $ 3 $ \\
 \hline
 $ 4 $ & $ 4 $ & $ -1 $ & $ 2 $ & $ 1 $ \\
 \hline
 $ 5 $ & $ 5 $ & $ -1 $ & $ 2 $ & $ 2 $ \\
 \hline
 \end{tabular} 
 \quad
 \begin{tabular}{ |c||c|c|c| } 

 \hline
 $ s $ \textbackslash $ \delta $ & $ \delta_1 $ & $ \delta_2 $ & $ \delta_3 $ \\ 
 \hline
 \hline
 $ 1 $ & $ 2 $ & $ 1 $ & $ 1 $  \\
  \hline
 $ 2 $ & $ 5 $ & $ 4 $ & $ 4 $  \\
 \hline
 
 \end{tabular}
 \end{center}
 $ \hfill \diamond $ 

\bigskip 
% SECTION: INFERENCE OF TURING MACHINES
\section{Inference of Turing Machines}
% TURING INFERENCE THEOREM
\textsc{Theorem} \; \textit{A deterministic Turing machine} $(\Lambda, Q, \Delta)$ \textit{can be strongly inferred by a device iff} 
$$\forall s \in S(U),\; |S^{-1}(s)| \geq 2. $$ \textit{This holds for both the representation of a Turing machine as a partial recursive function and the representation as an update function.} \\

\textsc{Proof} \; First examine the partial function case. Let $ f $ be the partial recursive function that describes the given Turing machine tuple. Let $ U := \N $. Choose any convenient single valued surjective function $ S: U \rightarrow \B^{*} $ and define $ T: U \rightarrow \B^{*} \cup \eta$ by $T(u) = f(S(u)) $ as the single-valued function mapping $ U $ to the halting and non-halting outputs of $ f $. Then $ f $ can be written as the single-valued mapping $ S \rightarrow T $ by $ f(s) = T(S^{-1}(s)) $. \\

Enumerate the elements of $ S(U) $ as $ 1, 2, \dots, s, \dots \; $. Let $ V^s = \{u : S^{-1}(s) = u \} $ for $ s \in S(U) $. Similarly enumerate the elements of $ V^s $ as $ s_1, s_2, \dots , s_{|V^{s}|}$. Then define $ X $ and $ Y $ as follows:

\begin{equation*}
X(s_i) = \begin{cases}
       a_s & \text{ if } i = 1 \\
       b_s & \text{ otherwise } \\
       \end{cases} \quad \quad 
Y(s_i) = \begin{cases}
       1 & \text{ if } i = 1 \\
       -1 & \text{ otherwise } \\
       \end{cases} 
\end{equation*}

\bigskip
Note that the condition $ |S^{-1}(s)| \geq 2 $ is required to guarantee $ Y(V^s) = \{1, -1\} $. For each pair $ (s, \delta_{t \in T(U)} ) $, to force $ S(u) = s $ and $ Y(u) = \delta_{t}(T(u)) $, choose $ x = a_s $ if $ t = T(s_1) $ or otherwise choose $ x = b_s $. Since the choices of $ s $ and $ t $ were arbitrary, this holds for all $ (s, \delta_t) $ pairs. \\

Now consider the update function that describes the given Turing machine. Recall that the update function is written as $ \Delta: Q \times \Lambda^{k} \rightarrow Q \times \Lambda^{k - 1} \times \{ L, \mathcal{S}, R \} ^{k} $, $ k \geq 2 $. Consider a convenient single-valued surjective function $ S: U \rightarrow Q \times \Lambda^{k} $ representing the possible inputs for a Turing Machine and a corresponding single-valued $ T: U \rightarrow Q \times \Lambda^{k - 1} \times \{ L, \mathcal{S}, R \}^ {k} $ as $ T(u) = \Delta(S(u)) $. Observe that $ \Delta $ can be written as the single-valued function $ \Delta(s) = T(S^{-1}(s)) $. Then define $ V^s $, $ X $, $ Y $, and choose $ x $ for each $ (s, \delta_t ) $ as described in the preceding portion of the proof. Hence, the claim holds for the update function of a Turing machine.\\

To show that the condition is necessary for either representation, suppose that $|V^s| < 2$ for some $ s $. If $ V^s = \emptyset $ then there exists no $ x $ that can force $ S = s $. If $ |V| = 1 $, then we can assign $ Y(s_i) = y \in \{-1, 1\} $. However, whichever value is assigned, there exists a $ t $ such that $ \delta_{t}(T(s_i)) \neq Y(s_i) $ since $ |T(U)| \geq 2 $. $ \hfill \qed $ \\


\textsc{Remark.} Conventionally all functions over $ U $ must have a range of at least two elements. This implies that Turing machines that never halt cannot be strongly inferred.


%\bigskip
%\bigskip
%\textsc{Corollary} \quad \textit{}



%\bigskip
%\bigskip
%\textsc {Theorem} \quad \textit{Any non-deterministic Turing Machine can be strongly inferred by a device if} 
%$$\forall s \in S(U),\; |S^{-1}(s)| \geq 2. $$
%\textit{This condition holds for both the partial function representation and the instantaneous description of the Turing Machines.} \\

% # NOTES #
% Don't need to consider partial function case
% Need to construct two update functions 
%Consider the two update functions $ \Delta_0 $, $ \Delta_1 : Q \times \Lambda^{k} \rightarrow Q \times \Lambda^{k - 1} \times \{ L, S, R \} ^{k} $, $ k \geq 2 $ of a nondeterministic Turing machine. Let $ U := \N $. Choose any convenient single-valued surjective function $ S: U \rightarrow Q \times \Lambda^{k} $ and a corresponding single-valued $ T: U \rightarrow Q \times \Lambda^{k - 1} \times \{ L, S, R \}^ {k} $ as:
%
%\begin{equation*}
% T(u) = \begin{cases}
%        \Delta_0(S(u)) \\
%        \Delta_1(S(u)) \\
%        \end{cases} \text{arbitrarily}
%\end{equation*} 
%       
%Then we write $ \Delta_{0}(s), \Delta_{1}(s) = T(S^{-1}(s)) $


% Does this mean that there can be different t for each u? 

% #########

% SECTION: INFERENCE COMPLEXITY
\section{Inference Complexity}

% INFERENCE COMPLEXITY DEFINITIONS
To examine analogues between inference devices and algorithmic information theory, there should be some measure of difficulty, or complexity, in a device's ability to weakly infer $ \Gamma $ or strongly infer $ f $. \\

\textsc{Definition} \; Let $ \mathcal{D} $ be an inference device and $ \Gamma $ be a function over $ U $ where $ X(U) $ and $ \Gamma(U) $ are countable and $ \mathcal{D} > \Gamma $. Let the \textbf{size} of $\gamma \in \Gamma(U) $ be written as $ \mathcal{M}_{\mu:\Gamma(\gamma)} = -\ln[\int_{\Gamma^{-1}(\gamma)} d\mu(u) 1] $ such that $ d\mu $ denotes a measure over $ U $. Then the \textbf{inference complexity} of $ \Gamma $ with respect to $ \mathcal{D} $ and measure $ \mu $ is defined as 

$$ \mathcal{C}_{\mu}(\Gamma ; \mathcal{D}) \triangleq \sum_{\delta \in \wp(\Gamma)} \min_{x : X = x \implies Y = \delta(\Gamma) } [\mathcal{M}_{\mu, X} (x)].	$$ 

The \textbf{strong inference complexity} for any single $ \gamma \in \Gamma $ is  

$$ \mathcal{C}_{\mu}(\gamma ; \mathcal{D}) \triangleq \min_{x : X(u) = x \implies S(u) = s, Y(u) = \delta_{\gamma}(\Gamma(u)) } [\mathcal{M}_{\mu, X} (x)].	$$ \\


We demonstrate an upper bound on the inference complexity of any set of countable functions.\\

% MAXIMUM WEAK INFERENCE THEOREM
\textsc{Theorem} \; \textit{Let c be a positive integer. There exists a countable set of inferable functions $ A^{*} $ and device $ \mathcal{D} $ that infers all functions $ a \in A^{*} $ such that every finite subset $ A \subseteq A^{*} $ with cardinality $ m $ has at least $  m(1 - 2^{- c - 1}) + 1 $ elements $ a \in A $ such that $ \mathcal{C}_{\mu}(a;\mathcal{D}) \geq - \ln (\log m - c) $ .} \\

\textsc{Proof} \; Choose $ A^{*} $ such that $ \overline{a(V)} $ is equivalent to $ \overline{A_5(V)} $ for $ a \in A^{*} $. Then choose any subset $ A \subseteq A^{*} $ with cardinality $ m $. Let $ U := \B^{*} $. Construct $ \mathcal{D} $ such that it weakly infers $ a \in A $. Take $ d\mu(u) = \ell(u) $ where $ \ell(b) $ is the length of a bit string $ b $. Recall that the inference complexity of $ a \in A $ with respect to $ \mathcal{D} $ is
$$ \mathcal{C}_{\mu}(a ; \mathcal{D}) = \sum_{\delta \in \wp(a)} \min_{x : X = x \implies Y = \delta(a) } [\mathcal{M}_{\mu, X} (x)]: \mathcal{M}_{\mu, X} (x) = - \ln \sum_{X^{-1}(x)} {\ell(u)} $$

By the construction of $ A^{*} $ and $ \mathcal{D} $, it requires at least two values of $ u $ to weakly infer each $ a $, and at least one of these $ u $ must be unique between all $ a $. Then the inference complexity of $ a $ with respect to $ \mathcal{D} $ is

$$ \mathcal{C}_{\mu}(a ; \mathcal{D}) = \sum_{\delta \in \wp(a)} \min_{x : X = x \implies Y = \delta(a_i) } -\ln \sum_{X^{-1}(x)} {\ell(X^{-1}(x))} $$

$$ = - \ln (\ell (X^{-1}(x)) + 1) = - \ln \ell(\overline{x} + 1). $$

Since a unique $ u $ is used by $ \mathcal{D} $ to infer every $ a \in A $, we can apply the pigeonhole principle. The number of bit strings of length less than $ \log m - c - 1 $ is 

$$ \sum_{i = 0}^{m - c - 2} {2^i} = 2^{\log m - c - 1} - 1. $$

By the pigeonhole principle, there are at least $ m - m2^{- c - 1} + 1 $ functions $ a \in A $ which have inference complexity greater than $ - \ln (\log m - c) $. \hfill \qed \\

Now we show that we can find a countably infinite set of functions such that the inference complexity of weakly inferring the entire set is zero. \\ 

% INFERENCE COMPLEXITY LOWER BOUND THEOREM
\textsc{Theorem} \; \textit{There exist $ A^{*} $ and $ \mathcal{D} $ such that for all functions of any subset $ A \subseteq A^{*} $, $ \mathcal{C}_{\mu}(A_i ; \mathcal{D}) = 0 $. } \\

\textsc{Proof} \; Recall the functions and spaces defined in the Countable Inference Theorem and take $ \mu(u) = \ell (u) $. Then choose $ A^{*}: \forall A_i(V) \in A^{*}, \overline{A_ i(V)} = \overline{A_{j \in \{2, 3, 4\}}(V)} $. Then the inference complexity for any function $ A_i \in A $ is 
$$ \mathcal{C}_{\mu}(A_i ; \mathcal{D}) = \sum_{\delta \in \wp(A_i)} \min_{x : X = x \implies Y = \delta(A_i) } - \ln \sum_{X^{-1}(x)} {\ell(u)} = \sum_{\delta \in \wp(f)} \min_{x : X = x \implies Y = \delta(A_i) } - \ln {1}  = - 2 \ln {1} = 0. $$
$ \hfill \qed $ \\

These results suggest the definition of inference complexity may need to be revised for it to more closely fit intuitive perceptions of knowledge and complexity.


% INCOMPRESSIBILITY
\section{Incompressibility}
% LI & VITANYI INCOMPRESSIBILITY THEOREM
One of the most immediate and interesting results from Kolmogorov complexity is the concept of incompressibility. \\

\textsc{Incompressibility Theorem} \; \textit{Let c be a positive integer. For each fixed y, every finite set A of cardinality m has at least} $ m(1 - 2^{-c}) + 1 $ \textit{elements x with} $ \mathcal{C}(x|y) \geq \log m - c $. \textit{This result is taken from Li and Vitanyi (1993)} [3]. \\

\textsc{Proof} \; The number of programs of length less than $ \log m - c $ is 

$$ \sum_{i = 0}^{\log m - c - 1} {2^i} = 2^{\log m - c} - 1. $$

Hence, there are at least $ m - m2^{-c} + 1 $ elements in $ A $ which have no program of length less than $ \log m - c $. $ \hfill \qed $ \\

% Look into log definition and Kraft's inequality, makes sure everything check outs okay 
\textit{Note.} Let $ \mathcal{M}_{\mu:\Gamma(\gamma)} = [\int_{\Gamma^{-1}(\gamma)} d\mu(u) 1] $ for the remainder of this section. The $ -\ln $ term is not needed because $ U $ will be taken over the space of bit strings instead of the natural numbers. \\

Similar incompressibility results hold over strong inference complexity. \\

\textsc{Theorem} \; \textit{There is a constant $ c $, such that $
\mathcal{C}_{\mu}(t;\mathcal{D}) \leq 2\ell(t) + c $  for all $ t $.} \\

\textsc{Proof} \; Let $ U := \B^{*} \times \{0, 1\} $. Fix a Turing machine $ \phi $ over $ U $ such that $ S: U \rightarrow \B^{*} $ is written as $ S((u, b)) = u $. Then define $ T: U \rightarrow \B^{*} $ as $ T(u) = S(u) $. Construct a device $ \mathcal{D} $ that strongly infers $ (S, T) $. Then for every $ t \in \B^{*} $, $ \mathcal{C}_{\mu}(t; \mathcal{D}) \leq 2\ell(t) + 2 $. \hfill \qed \\

% STRONG INFERENCE INCOMPRESSIBILITY RESULT

\textsc{Theorem} \; \textit{Let c be a positive integer. For a fixed $ \mathcal{D} $, every finite set $ T(U) $ of cardinality $ m $ has at least $ m(1 - 2^{-c - 1}) + \frac{1}{2} $ elements $ t $ with $ \mathcal{C}_{\mu}(t;\mathcal{D}) \geq \log m - c $. } \\

\textsc{Proof} \; Set $ U := \B^{*} $. Fix any universal partial recursive function $ \phi $ that can be strongly inferred. Let $ \mu(u) = \ell (u) $. Construct a device $ \mathcal{D} $ and functions $ (S, T) $ over $ \phi $ such that $ \mathcal{D} \gg (S, T) $. By the construction of $ \mathcal{D} $, it takes at least two unique values of $ u $ to force strong inference for each value $ t $. The number of bit strings of length less than $ \log m - c $ is 

$$ \sum_{i = 0}^{\log m - c - 1} {2^i} = 2^{\log m - c} - 1. $$

Then the greatest number of unique pairs of bit strings with combined length less or equal to $ \log m - c $ is $ m2^{- c - 1} - \frac{1}{2} $. Hence, there are at least $ m - m2^{-c - 1} + \frac{1}{2} $ values $ t $ with $ \mathcal{C}_{\mu}(t;\mathcal{D}) \geq \log m - c $. \hfill \qed \\ 
   
% INCOMPRESSIBILITY RESULT END

Furthermore, strong inference complexity is unbounded. \\

\textsc{Theorem} \; \textit{Consider $ \Gamma $ with countably infinite image over $ U $. (i) The function $ \mathcal{C}(\gamma;\mathcal{D}) $ is unbounded. (ii) Define a function $ m $ by $ m(\gamma) = \min\{ \mathcal{C}(\psi;\mathcal{D}) : \psi \geq \gamma \}$. That is, $ m $ is the greatest monotonic increasing function bounding $ \mathcal{C} $ from below. The function $ m(\gamma) $ is unbounded. } \\

%(iii) For any partial recursive function $ \phi(\gamma) $ which goes monotonic to infinity from some $ \gamma_0 $ onward, we have $ m(\gamma) < \phi(\gamma) $ except for finitely many $ \gamma $. In other words, although $ m(\gamma) $ goes to infinity, it does so slower than any unbounded partial function. } \\

\textsc{Proof} \; (i) This follows immediately from (ii). (ii). For each $ i $ there is a least $ \gamma_{i} $ such that for all $ \gamma > \gamma_{i} $ the smallest set of $ u \in U $ strongly inferring $ \gamma $ has a total length greater or equal to $ i $. This follows immediately from the fact that there are only a finite number of $ u $ of each length $ i $. Clearly, for all $ i $ we have $ \gamma_{i + 1} \geq \gamma_{i} $. Now observe that the function $ m $ has the property that $ m(\gamma) = i + 1 $ for $ \gamma_{i} < \gamma \leq \gamma_{i + 1} $. This proves (ii). 
\hfill \qed 


\section{Physical Knowledge}
% Physical Knowledge

A device can weakly infer a function without ever correctly answering "yes" to any of the questions posed to it. In other words, a device does not need to know when a value of physical variable is present for weak inference. The device only needs to know, for each question posed to it, at least one other value that is not present. This arises from the weakness of the definition of weak inference. To force a device to actually know a physical variable, we wish for the device to be able to infer when a value is present and when it is not. We call this stronger condition physical knowledge. \\

\textsc{Definition} \; Consider an inference device $ (X, Y) $ defined over $ U $, a function $ \Gamma $ defined over $ U $, a $ \gamma \in \Gamma(U) $, and a subset $ W \subseteq U $. We say that $ (X, Y) $ \textbf{(physically) knows} $ \Gamma = \gamma $ over $ W $ iff $\exists \xi : \Gamma(U) \rightarrow \overline{X} $ such that \\

\textit{i}) $ \forall \gamma' \in \Gamma(U), u \in \xi(\gamma') \implies \delta_{\gamma'}(\Gamma(u)) = Y(u) $ \\ 
\textit{ii}) $ \emptyset \neq \xi(\gamma) \cap W \subseteq Y^{-1}(1) $ \\
\textit{iii}) \textit{For all} $ \gamma' \neq \gamma $, $ \emptyset \neq \xi(\gamma') \cap W \subseteq Y^{-1}(-1) $. \\

A device $ \mathcal{D} $ physically knows a function $ \Gamma $ over $ W $ iff $ \forall \gamma \in \Gamma(U)$,  $ \exists \xi : \mathcal{D} $ knows $ \gamma $ over $ W \subseteq U $ using $ \xi $. \\


\textsc{Definition} \; The \textbf{knowledge complexity} of $ \Gamma $ given $ \mathcal{D} $, $\xi $, and measure $ \mu $ is defined as: 

$$ \mathcal{K}_{\mu}(\Gamma ; \mathcal{D}) \triangleq \mathcal{M}_{\mu} [\bigcup_{\gamma \in \Gamma(U)} \min_{\overline{\{\xi_i\}} : \mathcal{D} \text{ knows } \Gamma = \gamma \text{ using } \xi_i \in \xi^{*}}] $$ 

$$ \mathcal{M}_{\mu; \xi} (\gamma) = -\ln \int_{\xi(\gamma)} d\mu(u) 1 $$ \\


\textsc{Example.} \; Let $ U := \{1, 2, 3\} $ and take $ W = U $. The following table and formulas demonstrate physical knowledge for $ \gamma \in \Gamma : |\Gamma(U)| = 2 $. \\

\begin{tabular}{cc}
\begin{minipage}{0.45\textwidth}
    \begin{flushright}
         \begin{tabular}{ |c||c|c|c| } 
          \hline
          $ u $ & $ X(u) $ & $ Y(u) $ & $ \Gamma(u) $  \\ 
          \hline
          \hline
          $ 1 $ & $ 1 $ & $ 1 $ & $ 1 $ \\
          \hline
          $ 2 $ & $ 2 $ & $ -1 $ & $ 2 $ \\
          \hline
          $ 3 $ & $ 3 $ & $ 1 $ & $ 2 $ \\
          \hline
          $ 4 $ & $ 4 $ & $ -1 $ & $ 1 $ \\
          \hline
         \end{tabular}
   \end{flushright}
\end{minipage}&
\begin{minipage}{0.45\textwidth}

\begin{equation}
\xi_{1}(\gamma) = \begin{cases}
                      \{1\} & \text{ if } \gamma = 1 \\
                      \{4\} & \text{ if } \gamma = 2. \\
                \end{cases} \notag
\end{equation}
\begin{equation}
\xi_{2}(\gamma) = \begin{cases}
                      \{2\} & \text{ if } \gamma = 1 \\
                      \{3\} & \text{ if } \gamma = 2. \\
                \end{cases} \\ \notag
\end{equation} 

\end{minipage} 
\end{tabular}

$ \hfill \diamond $
\bigskip

\textsc{Lemma} \; \textit{For every countably-ranged function $ \Gamma $ there exists a device $ \mathcal{D} $ such that $ \mathcal{D} $ physically knows all $ \gamma \in \Gamma(U) $ over $ W = U $ if $ |\Gamma^{-1}(\gamma)| \geq 2 $ for all $ \gamma \in \Gamma $.} \\

\textsc{Proof} \; Take $ U := \N $ and $ W = U $. Choose any $ \Gamma $ as described in the claim. Enumerate $ \gamma \in \Gamma(U) $ as $ 1, 2, \dots, i, \dots $ . Define $ V^{i} = \{u : \Gamma^{-1}(\gamma) = u \} $ for each $ i \in \Gamma(U) $. Similarly enumerate each $ u \in V^{i} $ as $ i_1, i_2, \dots, i_j, \dots $ . Now define the device $ (X, Y) $ as 

\begin{equation*}
X(i_j) = \begin{cases}
       a_i & \text{ if } j = 1 \\
       b_i & \text{ if } j = 2 \\
       c_i & \text{ otherwise } \\
       \end{cases} \quad \quad 
Y(i_j) = \begin{cases}
       1 & \text{ if } j = 1 \\
       -1 & \text{ otherwise. } \\
       \end{cases} 
\end{equation*}

Define $ \xi_{\gamma} : \Gamma(U) \rightarrow \overline{X} $ for each $ i \in \Gamma(U) $  as

\begin{equation*}
\xi_{\gamma}(i) = \begin{cases}
       \overline{a_i} & \text{ if } i = \gamma \\
       \overline{b_i} & \text{ otherwise. } \\
       \end{cases} 
\end{equation*}

Then for all $ \gamma \in \Gamma(U) $ and all $ u \in \xi_{\gamma}(\gamma')$, $ \delta_{\gamma}(\Gamma(u)) = Y(u) $. Furthermore, for each $ \xi_{\gamma} $, $ \emptyset \neq \{ ( \xi_{\gamma}(\gamma' = \gamma) \cap W) \subseteq Y^{-1}(1) \} $ and $ \emptyset \neq \{ ( \xi_{\gamma}(\gamma' \neq \gamma) \cap  W ) \subseteq Y^{-1}(-1) \} $. Hence, $ \mathcal{D} $ physically knows $ \gamma \in \Gamma(U) $. \hfill \qed \\

We can extend this result to show that any set of functions can be physically known by a single device. \\

\textsc{Theorem} \; \textit{For any set of independently physically knowable characteristic functions $ A^{*} $ there is a device $ \mathcal{D} $ and set of functions $ \xi{*} $ such that $ \mathcal{D} $ knows $ A \in A^{*} $ over a given subset $ W \subseteq U $ using $ \xi^{*} $. } \\

\textsc{Proof} \; Let $ U := \N $ and take $ W = U $. Fix $ A^{*} $ and construct $ \mathcal{D} $ such that $ \mathcal{D} > A \in A^{*} $. For all $ A \in A^{*} $, or all $ \gamma \in A(U) $, define $ \xi_{\gamma} $ as specified in the preceding theorem. Then for all $ \gamma \in A(U) $ and all $ u \in \xi_{\gamma}(\gamma')$, $ \delta_{\gamma}(\Gamma(u)) = Y(u) $. Furthermore, for each $ \xi_{\gamma} $, $ \emptyset \neq \{ ( \xi_{\gamma}(\gamma' = \gamma) \cap W) \subseteq Y^{-1}(1) \} $ and $ \emptyset \neq \{ ( \xi_{\gamma}(\gamma' \neq \gamma) \cap  W ) \subseteq Y^{-1}(-1) \} $. Hence $ \mathcal{D} $ physically knows $ \gamma \in A(U) $ for $ A \in A^{*} $ . \hfill \qed \\

\bigskip
\textsc{Theorem} \; \textit{Let $ \Gamma $ be a knowable function with countable range and $ \mathcal{D} $ be a device that physically knows all $ \gamma \in \Gamma $ over $ U $. Every such $ \Gamma $ with $ |\Gamma(U)| = m $ has knowledge complexity of at most $ - \ln \sum_{i = 1}^{2m} {i} $ .} \\ 

\textsc{Proof} \; Set $ U := \N $. Fix $ \Gamma $ with $ | \Gamma(U) | = m $. Let $ \mu(u) = \ell (u) $. Construct a device $ \mathcal{D} $ and a set of functions $ \xi^{*} $ such that $ \mathcal{D} $ physically knows $ \gamma \in \Gamma $ over $ U $. By the construction of $ \mathcal{D} $, it takes at least $ 2m $ values of $ u $ to physically know each value $ \gamma $ over $ U $. Then the knowledge complexity of $ \Gamma $ with $ \mathcal{D} $ using $ \xi_{\gamma \in \Gamma(U)} $ is
$$ \mathcal{K}_{\mu}(\Gamma ; \mathcal{D}) = \mathcal{M}_{\mu}(\bigcup_{\gamma \in \Gamma(U)} \min_{\overline{\{\xi_i\}} : \mathcal{D} \text{ knows } \Gamma = \gamma \text{ using } \xi_i \in \xi^{*}})  = $$ 

$$ \mathcal{M}_{\mu} (\{\epsilon, \dots, 2m \}) = - \ln \sum_{i = 1}^{2m} {i} $$ 
\hfill \qed \\ 

\bigskip
\textsc{Theorem} \; \textit{There exists a countable set of inequivalent characteristic functions $ A^{*} $ and a device $ \mathcal{D} $ such that for all $ A \in A^{*} $, $ \mathcal{D} $ physically knows $ A $ over $ U $ and $ \mathcal{K}_{\mu}(A;\mathcal{D}) = - \ln 10 $. } \\

\textsc{Proof} \; Set $ U := \N $. Fix $ A^{*} $ such that for any $ A \in A^{*} $, $ A(1) = A(2) = 0 $ and $ A(2) = A(3) = 1 $. Let $ \mu(u) = \ell (u) $. Construct a device $ \mathcal{D} $ and a set of functions $ \xi^{*} $ such that $ \mathcal{D} $ physically knows $ a \in A $ over $ U $. By the construction of $ \mathcal{D} $, it takes only the values $ u = 1, 2, 3, 4 $ to physically know each $ A $ over $ U $. Then the knowledge complexity of all $ A $ with $ \mathcal{D} $ using $ \xi_{\gamma \in \Gamma(U)} $ is
$$ \mathcal{K}_{\mu}(\Gamma ; \mathcal{D}) = \mathcal{M}_{\mu}(\bigcup_{\gamma \in \Gamma(U)} \min_{\overline{\{\xi_i\}} : \mathcal{D} \text{ knows } \Gamma = \gamma \text{ using } \xi_i \in \xi^{*}}) = - \ln \sum_{i = 1}^{4}{i} = - \ln 10 $$
\hfill \qed 


% SECTION: OPEN QUESTIONS
\section{Open Questions}

Much of the results covered in this manuscript are preliminary and leaves much to be desired in our understanding of the theory of inference devices and its connections to algorithmic information theory. There are many immediate questions deserving attention. The question of whether non-deterministic Turing machines may be strongly inferred was left unattended. Our incompressibility results with strong inference of universal Turing machines may be extended by exploring possible analogues between Kolmogorov complexity and strong inference complexity. The numerous negative results we found concerning the definitions of weak inference complexity and physical knowledge suggest that a revisitation of these definitions may be appropriate or fruitful. Any revisal of these definitions should consider possible compatibility with the similar topics of Shannon information and prefix code.

% SECTION: ACKNOWLEDGEMENTS
\section{Acknowledgements}
I am grateful to David Wolpert for his mentorship and for many detailed discussions. I would like to thank the education office of the Santa Fe Institute (SFI) and the participants of the SFI Research Experience for Undergraduates (REU) program for providing valuable resources and assistance. This work was supported by the National Science Foundation, Grant No. 1757923. \\

\bigskip
\noindent\rule{\textwidth}{1pt} \\

\begin{enumerate}[label={[\arabic*]}]

\item Wolpert DH. \textit{Constraints on physical reality arising from a formalization of knowledge}. arXiv preprint arXiv:1711.03499v3 [physics.hist-ph] (2018). 
\item Li M \& Vitányi P. \textit{An Introduction to Kolmogorov Complexity and Its Applications} (1st ed.). Springer-Verlag. (1993). 
\item Arora S \& Barak B. \textit{Computational Complexity: A Modern Approach}. Cambridge University Press. (2009). 

\end{enumerate}

\end{document}

