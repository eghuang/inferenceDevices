\documentclass[]{article}
\usepackage[margin=1in]{geometry}          
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{csquotes}
\usepackage{setspace}\onehalfspacing
\usepackage[loose,nice]{units} %replace "nice" by "ugly" for units in upright fractions
\usepackage{graphicx}
\graphicspath{ {/Users/eghuang/math104/images/} }
\usepackage{listings}
\usepackage{color}
\usepackage{glossaries}	

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=R,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\title{
  Algorithmic Information and Inference Devices \\
  \bigskip
  \large Santa Fe Institute}
\author{Edward G. Huang}
\date{Summer 2018} 
 
 \newcommand{\R}{\mathbb{R}}
 \newcommand{\Q}{\mathbb{Q}}
 \newcommand{\Z}{\mathbb{Z}}
 \newcommand{\N}{\mathbb{N}}
 \newcommand{\B}{\mathbb{B}}
 \newcommand{\Prob}{\mathbb{P}}
 \newcommand{\E}{\mathbb{E}}
 \newcommand{\code}[1]{\texttt{#1}}
 \let\oldemptyset\emptyset
 \let\emptyset\varnothing
 
 \setlength{\parindent}{0pt} % no indent
 
\begin{document}
\maketitle 

% HOOPER: Please include an abstract, a brief intro with a clear statement of your research question(s), a description of your methods/model/approach, your results, and your conclusions (or lessons learned).

% SECTION 0: ABSTRACT
\section{Abstract}

There has been much interest surrounding what properties about the universe can be derived from applying a mathematical formalization of inference and knowledge. Previously, Wolpert demonstrated bounds on knowledge in any physical universe that allows agents to hold information concerning that universe by using the concept of "inference devices" (IDs). We extend previous work on the capacity and limitations of IDs to infer physical variables. We then explore analogues between IDs and their relation to Turing machine theory and algorithmic information theory.


% SECTION 1: INTRODUCTION
\section{Introduction} 

% SUBSECTION: NOTATION
\subsection{Notation and Definitions}
This manuscript utilizes standard notation taken from set theory and vector algebra. We clarify notation specific to Turing machine theory and inference devices. \\


\begin{tabular}{cp{0.6\textwidth}}
\textbf{Turing Machine Notation} \\

$ \B^{*} $ & The space of all finite bit strings, $ \{\epsilon, 0, 1, 00, 01, \dots \} $\\
$ \Lambda $ & Symbol alphabet of a Turing Machine. \\
$ \sigma  $ & A symbol on a Turing Machine tape. \\
$ Q $ & Set of finite states of a Turing Machine. \\
$ \Delta $ & Transition function of a Turing Machine. \\
$ k $ & Number of tapes of a Turing Machine. The first tape is assumed to be read-only. \\
$ \eta $ & Non-halting state of a Turing Machine. \\

\textbf{Inference Device Notation} \\
$ U $ & Set of possible histories of the universe. \\
$ u $ & A history of the universe in $ U $. \\ 
$ X $ & Setup function of an ID that maps $ U \rightarrow X(U) $. A binary question concerning $ \Gamma(u) $. \\
$ x $ & A binary question and a member of image $ X(U) $. \\ 
$ Y $ & Single-valued conclusion function of an ID that maps $ U \rightarrow \{-1, 1\} $. A binary answer of an ID for  $ X(u) = x $. \\ 
$ y $ & A single-valued answer, and member of image $ Y(U)  = \{0, 1\} $. \\ 
$ \Gamma $ & A function of the actual values of a physical variable over $U$, equivalent to $\Gamma(u) = S(t_i)(u)$.  \\
$ \gamma $ & Possible value of a physical variable, a member of the image $\Gamma(U)$. \\
$ \delta $ & Probe of any variable $V$ parameterized by $v \in V$ such that : 
	  \[ \delta_v (v') =
	  \begin{cases} 
       1 & \text{ if } v = v' \\
       -1 & \text{ otherwise } \\
      \end{cases}\] \\
$ \wp $ & Set of probes over $\Gamma(U)$. \\
$ \mathcal{D}  $ & An inference device, consisting of functions $ (X, Y) $. \\
$ \xi $ & A function $ \Gamma(U) \rightarrow \overline{X} $.  \\
$ \Gamma^{-1} $ & Inverse. Given a function $ \Gamma $ over $ U $, $\Gamma ^ {-1} = \bar{\Gamma} \equiv \{\{u : \Gamma(u) = \gamma \} : \gamma \in \Gamma(U) \} $. \\
$ \overline{\Gamma} $ & Given a function $ \Gamma $ over $ U $, the partition of $ U $ given by $ \Gamma^{-1} $.  \\
$ > $ & Weak inference: a device $\mathcal{D}$ weakly infers $\Gamma$ \textit{iff} $ \forall \gamma \in \Gamma(U), \exists x \in X(U) $ s.t. $ \forall u \in U $, 
$ X(u) = x \implies Y(u) = \delta_{\gamma}(\Gamma(u)) $.  \\
$ \gg $ & Strong inference: a device $ (X, Y) $ strongly infers a function $ (S, T) $ over $ U $ \textit{ iff } $\forall \delta \in \wp(T) $ and all $ s \in S(U) $, $ \exists x $ \textit{ such that } $ X(u) = x \implies S(u) = s, Y(u) = \delta(T(u)) $. \\
$ \mathcal{C}_{\mu}(\Gamma; \mathcal{D}) $ & Inference complexity. \\
$ \mu $ & Measure defined for $ u \in U $. \\

\end{tabular}

% SUBSECTION: INFERENCE DEVICE
\subsection{Inference Devices}

% SUBSECTION: TURING MACHINES
\subsection{Turing Machines} 

% SUBSECTION: DTM
\textbf{Deterministic Turing Machines} \\
 Arora and Barak denote a Turing Machine (TM) as $ T = (\Lambda, Q, \Delta) $ containing:
\begin{enumerate}
\item An \textit{alphabet} $ \Lambda $ of a finite set of symbols that $ T $'s tapes can contain. We assume that $ \Lambda $ contains a special blank symbol $ B $, start symbol $ S $, and the symbols 0 and 1. 
\item A finite set $ Q $ of possible states that $ T $'s register can be in. We assume that $ Q $ contains a special start state $ q_{s} $ and a special halt state $ q_{h} $. 
\item A transition function $ \Delta : Q \times \Lambda^{k} \rightarrow Q \times \Lambda^{k - 1} \times \{L, \mathcal{S}, R\}^{k} $, where $ k \geq 2$, describing the rules $ T $ use in performing each step. The set $\{L, \mathcal{S}, R\}$ denote the actions \textit{Left, Stay,} and \textit{Right}, respectively. 
\end{enumerate}

Suppose $ T $ is in state $ q \in Q $ and $ (\sigma_1, \sigma_2, \dots, \sigma_k) $ are the symbols on the $ k $ tapes. Then $ \Delta(q, (\sigma_1, \dots, \sigma_k)) = (q', (\sigma_{2}^{'}, \dots, \sigma_{k}^{'}), z) $ where $ z \in \{L, \mathcal{S}, R\}^k $ and at the next step the $ \sigma $ symbols in the last $ k - 1 $ tapes will be replaced by the $ \sigma' $ symbols, the machine will be in state $ q $, and the $ k $ heads will move \textit{Left, Right} or \textit{Stay}. This is illustrated in Figure 2.1. \\

\textbf{Figure 2.2.1. The transition function $ \Delta $ for a $ k $-tape Turing Machine}

 \begin{center}
 \begin{tabular}{ c|c|c|c||c|c|c|c|c } 
 \hline
 \multicolumn{4}{c||}{ $ (q, (\sigma_1, \dots, \sigma_k)) $ } & 
      \multicolumn{5}{c}{ $ (q', (\sigma_{2}^{'}, \dots, \sigma_{k}^{'}), z) $ } \\
 
 \hline
 \begin{tabular}[c]{@{}c@{}} Input \\ symbol \end{tabular} & 
 \begin{tabular}[c]{@{}c@{}} Work/output \\ symbol \\ read \end{tabular} & 
 $\dots $ &
 \begin{tabular}[c]{@{}c@{}} Current \\ state \end{tabular} &
 \begin{tabular}[c]{@{}c@{}} New \\ work/output \\ tape symbol \end{tabular} & 
 $ \dots $ &
 \begin{tabular}[c]{@{}c@{}} Move \\ work/output \\ tape \end{tabular} &
 $ \dots $ &
 \begin{tabular}[c]{@{}c@{}} New \\ state \end{tabular} \\ 
 
 \hline
 \hline
 $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ \\ 
 \hline
 $  \sigma_1 $ & $ \sigma_i $ & $ \ddots $ & $ q $ & $ \sigma_i^{'} $ & $ \ddots $ & $ z_i $ & $ \ddots $ & $ q^{'} $ \\ 
 \hline
 $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ \\ 
 \end{tabular}
 \end{center} 

\bigbreak

\textit{Remark}: $\Lambda$ can be reduced to $ \B = \{0, 1\} $ and $ k $ can be reduced to $ 1 $ without loss of computational power. Then, any Turing Machine can be expressed as a partial recursive function mapping $ \B^{*} \rightarrow \B^{*} \cup \eta $, where $ \eta $ is the undefined non-halting output. Since $ |\B^{*} \times \B^{*} \cup \eta | = | \N \times \N | = | \N | $, the set of all Turing Machines is countably infinite. \\

% SUBSECTION: NDTM
\textbf{Non-deterministic Turing Machines} \\
Non-deterministic Turing Machines (NDTM) differ from deterministic Turing Machines by having two transition functions $ \Delta_0, \Delta_1 $ and a special state $ q_{accept} $. From Arora and Barak:

\begin{displayquote}
When a NDTM $M$ computes a function, we envision that at each computational step $ M $ makes an arbitrary choice as to which of its two transition functions to apply. For every input $ x $, we say that $ M(x) = 1 $ if there exists some sequence of these choices (which we call nondeterministic choices of $ M $) that would make $ M $ reach $ q_{accept} $ on input $ x $. Otherwise - if every sequence of choices makes $ M $ halt without reaching $ q_{accept} $ - then we say that $ M(x) = 0 $. 
\end{displayquote}

If $ M(x) = 1$, we say that $ M $ accepts the input $ x $. There are two ways to interpret the choice of update function to use in a NDTM. We can either assume that the NDTM chooses updates that will lead to an accepting state, or we can assume that the machine branches out into its choices such that it has a "computation tree" and if any of the branches reaches the accepting state then the machine accepts the input. From this second interpretation, the computational power of DTMs to NDTMs is analogous to the computational complexity of P to NP. \\

% SUBSECTION: UTM
\textbf{Universal Turing Machines}

% SECTION: WEAK INFERENCE
\newpage
\section{Weak Inference}
\bigskip   

\textbf{Definition} \quad Two functions $ \Gamma_1 $ and $ \Gamma_2 $ with the same domain $ U $ are \textbf{(functionally) equivalent} iff the inverse functions $ \Gamma^{-1}_1 $ and $ \Gamma^{-1}_2 $ induce the same partitions of $ U $, i.e., iff $ \overline{\Gamma_1} = \overline{\Gamma_2} $. \\

% LEMMA 1

\textbf{Lemma} \; \textit{A function} $ \Gamma $ \textit{can be weakly inferred by a device} $ \mathcal{D} $ \textit{if} $ |\Gamma^{-1}(\gamma)| \geq 2 $ \textit{for any} $ \gamma \in \Gamma(U) $. \\
\textbf{Proof} \; Let $ U := \N $. Enumerate $ \gamma \in \Gamma $ as $ 1, 2, \dots, i, \dots, n $ and define $ V^{i} = \{u: \Gamma^{-1}(i) = u \} $. Then enumerate each element in $ V^i $ as $ i_1, i_2, \dots, i_j, \dots, i_m $. Continue to define $ X $ and $ Y $ as 

\begin{equation*}
X(i_j) = \begin{cases}
       a_i & \text{ if } j = 2 \\
       b_i & \text{ otherwise } \\
       \end{cases} \quad \quad 
Y(i_j) = \begin{cases}
       -1 & \text{ if } j = 2 \\
       1 & \text{ otherwise. } \\
       \end{cases} 
\end{equation*}

Then for each $ i \in \Gamma(U) $ choose $ x = b_i $ to force $ Y(X^{-1}(b_i)) = \delta_{i}(\Gamma(X^{-1}(b_i)) = i) = 1. \hfill \qed $ \\

% COROLLARY 1
\bigskip   
\textbf{Corollary} \; \textit{A function} $ \Gamma $ \textit{can be weakly inferred by a device} $ \mathcal{D} $ \textit{if} $ |\Gamma(U)| \geq 3 $.  \\
\textbf{Proof} \; Let $ U := \N $. Enumerate $ \gamma \in \Gamma(U) $ as $ 1, 2, \dots, i, \dots, n $ and define $ V^{i} = \{u: \Gamma^{-1}(i) = u \} $. Then enumerate each element in $ V^i $ as $ i_1, i_2, \dots, i_j, \dots, i_m $. Continue to define $ X $ and $ Y $ as 

\begin{equation*}
X(i_j) = \begin{cases}
       a & \text{ if } i = 1 \text{ and } j = 1 \\
       b & \text{ if } i = 2 \text{ and } j = 1 \\
       c & \text{ if } i = 3 \text{ and } j = 1 \\
       d & \text{ otherwise. } \\
       \end{cases} \quad \quad 
Y(i_j) = \begin{cases}
       1 & \text{ if } i = 1 \text{ and } j = 1 \\
       -1 & \text{ otherwise. } \\
       \end{cases} 
\end{equation*}

Then for each $ i \in \Gamma(U) $ to force $ Y(X^{-1}(x)) = \delta_{i}(\Gamma(X^{-1}(x)) = i) $ choose $ x = a $ if $ i = 1 $, $ x = c $ if $ i = 2 $, or else choose $ x = b $. $ \hfill \qed $ \\

% COUNTABLE INFERENCE THEOREM
\bigskip
\newpage
\textbf{Countable Inference Theorem} \; \textit{A countable set of inequivalent functions} $ A^{*} $ \textit{can be weakly inferred by a device if each function $ A_i \in A^{*} $ is independently inferrable. } \\

\textbf{Proof} \; Let $ U := \N $ and fix any $ A^{*} $. Let $ a_i, a_j, a_k $ represent any distinct three elements in $ A_i(U) $. Write $ V = \{1, 2, 3\} \subset U $. The following table represents all possible combinations of $ a_i $, $ a_j$, and $ a_k $ over $ V $. \\
 \begin{center}
 \begin{tabular}{ |c||c|c|c|c|c|c|c|c } 

 \hline
 $ u $ & $ X(u) $ & $Y(u)$ & $ A_1(u) $ & $ A_2(u) $ & $ A_3(u) $ & $ A_4(u) $ & $ A_5(u) $ & $ \dots $ \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ -1 $ & $ a_i $ & $ a_i $ & $ a_i $ & $ a_i $ & $ a_i $ & $ \dots $ \\
 \hline
 $ 2 $ & $ 2 $ & $ -1 $ & $ a_i $ & $ a_j $ & $ a_j $ & $ a_j $ & $ a_i $ & $ \dots $ \\
 \hline
 $ 3 $ & $ 3 $ & $ 1 $ & $ a_i $ & $ a_k $ & $ a_j $ & $ a_i $ & $ a_j $ & $ \dots $ \\
 \hline
 $ \vdots $ & $ \vdots $ & $ \vdots $ & $ \vdots $ & $ \vdots $ & $ \vdots $ & $ \vdots $ & $ \vdots $ & $ \ddots $ \\
 \end{tabular} 
 \end{center}
\bigskip 
Note that setting $ X(u) = u $, $ Y(1) = -1 $, $ Y(2) = -1 $, and $ Y(3) = 1 $ immediately satisfies weak inference for any function $ A_i $ that is functionally equivalent to $ A_1 $, $ A_2 $, $ A_3 $ and $ A_4 $ over $ V $. This holds regardless of the values of $ Y(u) $ and $ A_{ i \in \{1, 2, 3, 4\} }(u) $ that follow for $ u > 3 $. This is shown by selecting $ x $ for the following cases:

\bigskip
\textit{Case 1:} $ \overline{A_i(V)} = \overline{A_1(V)} $. \\
 Choose $ x = 3 $ for $ a = a_i $ or otherwise choose $ x = 2 $. \\

\textit{Cases 2, 3, 4:} $ \overline{A_i(V)} = \overline{A_2(V)} $ or  $ \overline{A_3(V)} $ or $ \overline{A_4(V)} $. \\
 Choose $ x = 2 $ for $ a = a_i $ or otherwise choose $ x = 1 $. \\

Now we need to guarantee weak inference for $ A_i $ that are functionally equivalent to $ A_5 $ over $ V $. Enumerate each $ A_i \in A^{*}: \overline{A_i(V)} = \overline{A_5(V)} $ as $ B_4, B_5, \dots, B_i, \dots, B_n $. To satisfy weak inference for any $ A_i $ and $ B_i $, define $ X $, $ Y $ more explicitly as 
\begin{equation*} 
     X(u) = u, \quad \quad
     Y(u) = \begin{cases} 
    -1 & \text{ if } u = 1, 2 \text { or if } B_i(u) = a_j, a_k: X(u) = i \\
     1 & \text{ if } u = 3 \text{ or if } B_i(u) = a_i: X(u) = i \\
    -1 & \text { otherwise. } \\
            \end{cases} \\
\end{equation*}

For each $ a \in B_i(U) $ to force $ Y(X^{-1}(x)) = \delta_{a}(B(X^{-1}(x))) $ choose $ x = i $ if $ a = a_i $ or otherwise choose $ x = 1 $. \hfill \qed  \\


% SECTION 3: STRONG INFERENCE
\newpage
\section{Strong Inference} 
 
In the next three examples we examine strong inference of integer-valued functions. \\
 
% EXAMPLE 
 \textbf{Example} \quad Let $ T(U) = \{0 , 1\} $ and $ S(U) = \{0, 1, 2\} $. We construct $ (X, Y) $ in the table at the left such that it strongly infers $ (S, T) $. The right table indicates $ x $ for each $ s $, $ \delta $ such that the definition of strong inference is satisfied: \\ 
 \begin {center}
 \begin{tabular}{ |c||c|c|c|c| } 

 \hline
 $ u $ & $ X(u) $ & $ Y(u) $ & $ S(u) $ & $ T(u) $ \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ 1 $ & $ 0 $ & $ 0 $ \\
 \hline
 $ 2 $ & $ 2 $ & $ -1 $ & $ 0 $ & $ 0 $ \\
 \hline
 $ 3 $ & $ 3 $ & $ 1 $ & $ 1 $ & $ 0 $ \\
 \hline
 $ 4 $ & $ 4 $ & $ -1 $ & $ 1 $ & $ 0 $ \\
 \hline 
 $ 5 $ & $ 5 $ & $ 1 $ & $ 2 $ & $ 1 $ \\
 \hline 
 $ 6 $ & $ 6 $ & $ -1 $ & $ 2 $ & $ 1 $ \\
 \hline
 \end{tabular} 
 \quad 
 \begin{tabular}{ |c||c|c| } 

 \hline
 $ s $ \textbackslash $ \delta $ & $ \delta_0 $ & $ \delta_1 $ \\ 
 \hline
 \hline
 $ 0 $ & $ 1 $ & $ 2 $  \\
 \hline
 $ 1 $ & $ 3 $ & $ 4 $ \\
 \hline
 $ 2 $ & $ 6 $ & $ 5 $ \\
 \hline
 
 \end{tabular}
 \end{center}
 
% EXAMPLE  
 \bigskip
 \bigskip 
 \textbf{Example} \quad Let $ T(U) = \{1, 2, 3\} $ and $ S(U) = \{1, 2, 3, 4, 5\} $. Again, we construct $ (X, Y) $ in the table at the left such that it strongly infers $ (S, T) $. The right table indicates $ x $ for each $ s $, $ \delta $ such that the definition of strong inference is satisfied: \\ 

 \begin{center}
 \begin{tabular}{ |c||c|c|c|c| } 

 \hline
 $ u $ & $ X(u) $ & $ Y(u) $ & $ S(u) $ & $ T(u) $ \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ 1 $ & $ 1 $ & $ 1 $ \\
 \hline
 $ 2 $ & $ 2 $ & $ -1 $ & $ 1 $ & $ 1 $ \\
 \hline
 $ 3 $ & $ 3 $ & $ 1 $ & $ 2 $ & $ 1 $ \\
 \hline
 $ 4 $ & $ 4 $ & $ -1 $ & $ 2 $ & $ 1 $ \\
 \hline
 $ 5 $ & $ 5 $ & $ 1 $ & $ 3 $ & $ 2 $ \\
 \hline
 $ 6 $ & $ 6 $ & $ -1 $ & $ 3 $ & $ 2 $ \\
 \hline
 $ 7 $ & $ 7 $ & $ 1 $ & $ 4 $ & $ 2 $ \\
 \hline
 $ 8 $ & $ 8 $ & $ -1 $ & $ 4 $ & $ 2 $ \\
 \hline
 $ 9 $ & $ 9 $ & $ 1 $ & $ 5 $ & $ 3 $ \\
 \hline
 $ 10 $ & $ 10 $ & $ -1 $ & $ 5 $ & $ 3 $ \\
 \hline 
 \end{tabular} 
 \quad
 \begin{tabular}{ |c||c|c|c| } 

 \hline
 $ s $ \textbackslash $ \delta $ & $ \delta_1 $ & $ \delta_2 $ & $ \delta_3 $  \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ 2 $ & $ 2 $ \\
 \hline
 $ 2 $ & $ 3 $ & $ 4 $ & $ 4 $ \\
 \hline
 $ 3 $ & $ 6 $ & $ 5 $ & $ 6 $ \\
 \hline
 $ 4 $ & $ 8 $ & $ 7 $ & $ 8 $ \\
 \hline 
 $ 5 $ & $ 10 $ & $ 10 $ & $ 9 $ \\
 \hline
 
 \end{tabular}
 \end{center} 

% EXAMPLE
 \bigskip
 \bigskip 
 \textbf{Example} \quad Let $ T(U) = \{1, 2, 3\} $ and $ S(U) = \{1, 2\} $. In this example, the inferred function $ f: S \rightarrow T, f(s) = T(S^{-1}(s)) $ is not single-valued. \\ 

 \begin{center}
 \begin{tabular}{ |c||c|c|c|c| } 

 \hline
 $ u $ & $ X(u) $ & $ Y(u) $ & $ S(u) $ & $ T(u) $ \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ -1 $ & $ 1 $ & $ 1 $ \\
 \hline
 $ 2 $ & $ 2 $ & $ -1 $ & $ 1 $ & $ 2 $ \\
 \hline
 $ 3 $ & $ 3 $ & $ 1 $ & $ 1 $ & $ 3 $ \\
 \hline
 $ 4 $ & $ 4 $ & $ -1 $ & $ 2 $ & $ 1 $ \\
 \hline
 $ 5 $ & $ 5 $ & $ -1 $ & $ 2 $ & $ 2 $ \\
 \hline
 \end{tabular} 
 \quad
 \begin{tabular}{ |c||c|c|c| } 

 \hline
 $ s $ \textbackslash $ \delta $ & $ \delta_1 $ & $ \delta_2 $ & $ \delta_3 $ \\ 
 \hline
 \hline
 $ 1 $ & $ 2 $ & $ 1 $ & $ 1 $  \\
  \hline
 $ 2 $ & $ 5 $ & $ 4 $ & $ 4 $  \\
 \hline
 
 \end{tabular}
 \end{center} 

\bigskip 
\bigskip
\newpage
% SECTION: INFERENCE OF TURING MACHINES
\section{Inference of Turing Machines}
% TURING INFERENCE THEOREM
\textbf{Theorem} \quad \textit{A deterministic Turing machine} $(\Lambda, Q, \Delta)$ \textit{can be strongly inferred by a device iff} 
$$\forall s \in S(U),\; |S^{-1}(s)| \geq 2. $$ \textit{This holds for both the representation of a Turing machine as a partial recursive function and the representation as an update function.} \\

\textbf{Proof} \quad First examine the partial function case. Let $ f $ be the partial recursive function that describes the given Turing machine tuple. Let $ U := \N $. Choose any convenient single valued surjective function $ S: U \rightarrow \B^{*} $ and define $ T: U \rightarrow \B^{*} \cup \eta$ by $T(u) = f(S(u)) $ as the single-valued function mapping $ U $ to the halting and non-halting outputs of $ f $. Then $ f $ can be written as the single-valued mapping $ S \rightarrow T $ by $ f(s) = T(S^{-1}(s)) $. \\

Enumerate the elements of $ S(U) $ as $ 1, 2, \dots, s, \dots \; $. Let $ V^s = \{u : S^{-1}(s) = u \} $ for $ s \in S(U) $. Similarly enumerate the elements of $ V^s $ as $ s_1, s_2, \dots , s_{|V^{s}|}$. Then define $ X $ and $ Y $ as follows:

\begin{equation*}
X(s_i) = \begin{cases}
       a_s & \text{ if } i = 1 \\
       b_s & \text{ otherwise } \\
       \end{cases} \quad \quad 
Y(s_i) = \begin{cases}
       1 & \text{ if } i = 1 \\
       -1 & \text{ otherwise } \\
       \end{cases} 
\end{equation*}

\bigskip
Note that the condition $ |S^{-1}(s)| \geq 2 $ is required to guarantee $ Y(V^s) = \{1, -1\} $. For each pair $ (s, \delta_{t \in T(U)} ) $, to force $ S(u) = s $ and $ Y(u) = \delta_{t}(T(u)) $, choose $ x = a_s $ if $ t = T(s_1) $ or otherwise choose $ x = b_s $. Since the choices of $ s $ and $ t $ were arbitrary, this holds for all $ (s, \delta_t) $ pairs. \\

Now consider the update function that describes the given Turing machine. Recall that the update function is written as $ \Delta: Q \times \Lambda^{k} \rightarrow Q \times \Lambda^{k - 1} \times \{ L, \mathcal{S}, R \} ^{k} $, $ k \geq 2 $. Consider a convenient single-valued surjective function $ S: U \rightarrow Q \times \Lambda^{k} $ representing the possible inputs for a Turing Machine and a corresponding single-valued $ T: U \rightarrow Q \times \Lambda^{k - 1} \times \{ L, \mathcal{S}, R \}^ {k} $ as $ T(u) = \Delta(S(u)) $. Observe that $ \Delta $ can be written as the single-valued function $ \Delta(s) = T(S^{-1}(s)) $. Then define $ V^s $, $ X $, $ Y $, and choose $ x $ for each $ (s, \delta_t ) $ as described in the preceding portion of the proof. Hence, the claim holds for the update function of a Turing machine.\\

To show that the condition is necessary for either representation, suppose that $|V^s| < 2$ for some $ s $. If $ V^s = \emptyset $ then there exists no $ x $ that can force $ S = s $. If $ |V| = 1 $, then we can assign $ Y(s_i) = y \in \{-1, 1\} $. However, whichever value is assigned, there exists a $ t $ such that $ \delta_{t}(T(s_i)) \neq Y(s_i) $ since $ |T(U)| \geq 2 $. $ \hfill \qed $ \\


\textit{Remark:} Conventionally all functions over $ U $ must have a range of at least two elements. This implies that Turing machines that never halt cannot be strongly inferred.


%\bigskip
%\bigskip
%\textbf{Corollary} \quad \textit{}



%\bigskip
%\bigskip
%\textbf {Theorem} \quad \textit{Any non-deterministic Turing Machine can be strongly inferred by a device if} 
%$$\forall s \in S(U),\; |S^{-1}(s)| \geq 2. $$
%\textit{This condition holds for both the partial function representation and the instantaneous description of the Turing Machines.} \\

% # NOTES #
% Don't need to consider partial function case
% Need to construct two update functions 
%Consider the two update functions $ \Delta_0 $, $ \Delta_1 : Q \times \Lambda^{k} \rightarrow Q \times \Lambda^{k - 1} \times \{ L, S, R \} ^{k} $, $ k \geq 2 $ of a nondeterministic Turing machine. Let $ U := \N $. Choose any convenient single-valued surjective function $ S: U \rightarrow Q \times \Lambda^{k} $ and a corresponding single-valued $ T: U \rightarrow Q \times \Lambda^{k - 1} \times \{ L, S, R \}^ {k} $ as:
%
%\begin{equation*}
% T(u) = \begin{cases}
%        \Delta_0(S(u)) \\
%        \Delta_1(S(u)) \\
%        \end{cases} \text{arbitrarily}
%\end{equation*} 
%       
%Then we write $ \Delta_{0}(s), \Delta_{1}(s) = T(S^{-1}(s)) $


% Does this mean that there can be different t for each u? 

% #########

% SECTION: INFERENCE COMPLEXITY
\newpage
\section{Inference Complexity}

\textbf{Definition} \quad Let $ \mathcal{D} $ be an inference device and $ \Gamma $ be a function over $ U $ where $ X(U) $ and $ \Gamma(U) $ are countable and $ \mathcal{D} > \Gamma $. Let the \textbf{size} of $\gamma \in \Gamma(U) $ be written as $ \mathcal{M}_{\mu:\Gamma(\gamma)} = -\ln[\int_{\Gamma^{-1}(\gamma)} d\mu(u) 1] $ such that $ d\mu $ denotes a measure over $ U $. Then the \textbf{inference complexity} of $ \Gamma $ with respect to $ \mathcal{D} $ and measure $ \mu $ is defined as 

$$ \mathcal{C}_{\mu}(\Gamma ; \mathcal{D}) \triangleq \sum_{\delta \in \wp(\Gamma)} \min_{x : X = x \implies Y = \delta(\Gamma) } [\mathcal{M}_{\mu, X} (x)].	$$ 

The \textbf{strong inference complexity} for any single $ \gamma \in \Gamma $ is  

$$ \mathcal{C}_{\mu}(\gamma ; \mathcal{D}) \triangleq \min_{x : X(u) = x \implies S(u) = s, Y(u) = \delta_{\gamma}(\Gamma(u)) } [\mathcal{M}_{\mu, X} (x)].	$$ 

\bigskip
% INCOMPRESSIBILITY THEOREM
\textbf{Incompressibility Theorem} \quad \textit{Let c be a positive integer. For each fixed y, every finite set A of cardinality m has at least} $ m(1 - 2^{-c}) + 1 $ \textit{elements x with} $ \mathcal{C}(x|y) \geq \log m - c $. \\
\textbf{Proof} \quad The number of programs of length less than $ \log m - c $ is 

$$ \sum_{i = 0}^{\log m - c - 1} {2^i} = 2^{\log m - c} - 1. $$

Hence, there are at least $ m - m2^{-c} + 1 $ elements in $ A $ which have no program of length less than $ \log m - c $. $ \hfill \qed $ \\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Strong Inference Incompressibility Theorem} \; \textit{Let c be a positive integer. For a fixed $ \mathcal{D} $, every finite set $ T(U) $ of cardinality $ m $ has at least $ m - m2^{-c - 1} + \frac{1}{2} $ elements $ t $ with $ \mathcal{C}_{\mu}(t;\mathcal{D}) \geq \log m - c $. } \\

\textbf{Proof} \; \textit{Note:} Let $ \mathcal{M}_{\mu:\Gamma(\gamma)} = [\int_{\Gamma^{-1}(\gamma)} d\mu(u) 1] $ for this proof. Set $ U := \B^{*} $. Fix any universal partial recursive function $ \phi $ that can be strongly inferred. Let $ \mu(u) = \ell (u) $. Construct a device $ \mathcal{D} $ and functions $ (S, T) $ over $ \phi $ such that $ \mathcal{D} \gg (S, T) $. By the construction of $ \mathcal{D} $, it takes at least two unique values of $ u $ to force strong inference for each value $ t $. The number of bit strings of length less than $ \log m - c $ is 

$$ \sum_{i = 0}^{\log m - c - 1} {2^i} = 2^{\log m - c} - 1. $$

Then the greatest number of unique pairs of bit strings with combined length less or equal to $ \log m - c $ is $ m2^{- c - 1} - \frac{1}{2} $. Hence, there are at least $ m - m2^{-c - 1} + \frac{1}{2} $ values $ t $ with $ \mathcal{C}_{\mu}(t;\mathcal{D}) \geq \log m - c $. \hfill \qed \\ 
   

\newpage
\bigskip
\textbf{Inference Incompressibility Theorem} \quad \textit{Let c be a positive integer. Let $ A^{*} $ be a countable set of inferable functions and $ \mathcal{D} $ be a device that infers all functions $ a \in A^{*} $. There exists $ A^{*} $ and $ \mathcal{D} $ such that every finite subset $ A $ of $ A^{*} $ with cardinality m has at least $ m(1 - m^{-c}) + 1 $ elements $ a \in A $ such that $ \mathcal{C}_{\mu}(a;\mathcal{D}) \geq \log m - c $ .} \\

\textbf{Proof} \quad Fix any convenient $ A^{*} $ and choose any subset $ A \subseteq A^{*} $ with cardinality $ m $. Let $ U := \B^{*} $. Let $ X: U \rightarrow \B^{*} $ and $ Y:U \rightarrow \{-1, 1\} $ be the $ X $ and $ Y $ defined in the Countable Inference Theorem when given $ A $. Note that $ X $ in this case maps $ \N \rightarrow \B^{*} $ instead of $ \N \rightarrow \N $ but the mappings are equivalent. Then $ \mathcal{D} = (X, Y) > A $ by construction.

\bigskip
Take $ d\mu(u) = \ell(u) $ where $ \ell(b) $ is the length of a bit string $ b $. Recall that the inference complexity of $ a \in A $ with respect to $ \mathcal{D} $ is
$$ \mathcal{C}_{\mu}(a ; \mathcal{D}) = \sum_{\delta \in \wp(a)} \min_{x : X = x \implies Y = \delta(a) } [\mathcal{M}_{\mu, X} (x)]: \mathcal{M}_{\mu, X} (x) = - \ln \sum_{X^{-1}(x)} {\ell(u)} $$

Now suppose that all $ A_i \in A $ is functionally equivalent to $ A_5 $ over $ V $. Then the maximum inference complexity of any $ A_i $ with respect to $ \mathcal{D} $ is
$$ \mathcal{C}_{\mu}(A_i ; \mathcal{D}) = \sum_{\delta \in \wp(A_i)} \min_{x : X = x \implies Y = \delta(a) } -\ln \sum_{X^{-1}(x)} {\ell(A_i^{-1}(a))} $$

$$ = \sum_{\delta \in \wp(A_i)} \min_{x : X = x \implies Y = \delta(a) } - \ln {A_i^{-1}(a)} = - \ln 1 - \ln(A_i^{-1}(a_i))  = - \ln(u). $$

Since a unique $ u $ is used by $ \mathcal{D} $ to infer every $ A_i \in A $, we can apply the pigeonhole principle. The number of bit strings of length less than $ m - c $ is 
$$ \sum_{i = 0}^{m - c - 1} {2^i} = 2^{m - c - 1}. $$

Hence, there are at least $ m - m2^{-c} + 1 $ functions $ a $ in $ A $ which have inference complexity greater than $ \ln \log m - c $.  \\


\textbf{Corollary} \; \textit{ There exist $ A^{*} $ and $ \mathcal{D} $ such that for all functions of any subset $ A \subseteq A^{*} $, $ \mathcal{C}_{\mu}(A_i ; \mathcal{D}) = 0 $ } \\
\textbf{Proof} \quad 
Recall the functions and spaces defined in the Countable Inference Theorem and take $ \mu(u) = \ell (u) $. Then choose $ A^{*}: \overline{A_ i(V)} \in A^{*} = \overline{A_{j \in \{2, 3, 4\}}(V)} $. Then the inference complexity for any function $ A_i \in A $ is 
$$ \mathcal{C}_{\mu}(A_i ; \mathcal{D}) = \sum_{\delta \in \wp(A_i)} \min_{x : X = x \implies Y = \delta(A_i) } - 2 \ln \sum_{X^{-1}(x)} {\ell(u)} = \sum_{\delta \in \wp(f)} \min_{x : X = x \implies Y = \delta(A_i) } - \ln {1}  = - 2 \ln {1} = 0. $$
$ \hfill \qed $ 

\newpage
\section{Physical Knowledge}
% Physical Knowledge

\textbf{Definition} \; Consider an inference device $ (X, Y) $ defined over $ U $, a function $ \Gamma $ defined over $ U $, a $ \gamma \in \Gamma(U) $, and a subset $ W \subseteq U $. We say that $ (X, Y) $ \textbf{(physically) knows} $ \Gamma = \gamma $ over $ W $ iff $\exists \xi : \Gamma(U) \rightarrow \overline{X} $ such that \\

\textit{i}) $ \forall \gamma' \in \Gamma(U), u \in \xi(\gamma') \implies \delta_{\gamma'}(\Gamma(u)) = Y(u) $ \\ 
\textit{ii}) $ \emptyset \neq \xi(\gamma) \cap W \subseteq Y^{-1}(1) $ \\
\textit{iii}) \textit{For all} $ \gamma' \neq \gamma $, $ \emptyset \neq \xi(\gamma') \cap W \subseteq Y^{-1}(-1) $. \\

A device $ \mathcal{D} $ physically knows a function $ \Gamma $ over $ W $ iff $ \forall \gamma \in \Gamma(U)$,  $ \exists \xi : \mathcal{D} $ knows $ \gamma $ over $ W \subseteq U $ using $ \xi $. \\


\textbf{Definition} \; The \textbf{knowledge complexity} of $ \Gamma $ given $ \mathcal{D} $, $\xi $, and measure $ \mu $ is defined as: 

$$ \mathcal{K}_{\mu}(\Gamma ; \mathcal{D}) \triangleq \sum_{\gamma \in \Gamma(U)} \min_{\xi(\gamma) : \mathcal{D} \text{ knows } \Gamma = \gamma \text{ using } \xi} [\mathcal{M}_{\mu; \xi} (\gamma)]	$$ 

$$ \mathcal{M}_{\mu; \xi} (\gamma) = -\ln \int_{\xi(\gamma)} d\mu(u) 1 $$ \\


\textbf{Example} \quad Let $ U := \{1, 2, 3\} $ and take $ W = U $. The following table and formulas demonstrate physical knowledge for $ \gamma \in \Gamma : |\Gamma(U)| = 2 $. \\

\begin{tabular}{cc}
\begin{minipage}{0.45\textwidth}
    \begin{flushright}
         \begin{tabular}{ |c||c|c|c| } 
          \hline
          $ u $ & $ X(u) $ & $ Y(u) $ & $ \Gamma(u) $  \\ 
          \hline
          \hline
          $ 1 $ & $ 1 $ & $ 1 $ & $ 1 $ \\
          \hline
          $ 2 $ & $ 2 $ & $ -1 $ & $ 2 $ \\
          \hline
          $ 3 $ & $ 3 $ & $ 1 $ & $ 2 $ \\
          \hline
          $ 4 $ & $ 4 $ & $ -1 $ & $ 1 $ \\
          \hline
         \end{tabular}
   \end{flushright}
\end{minipage}&
\begin{minipage}{0.45\textwidth}

\begin{equation}
\xi_(\gamma) = \begin{cases}
                      \{1, 2\} & \text{ if } \gamma = 1 \\
                      \{3, 4\} & \text{ if } \gamma = 2. \\
                  \end{cases} \notag \\
\end{equation} 

\end{minipage} 
\end{tabular}

\bigskip
\bigskip
\textbf{Lemma} \; \textit{For every countably-ranged function $ \Gamma $ there exists a device $ \mathcal{D} $ such that $ \mathcal{D} $ physically knows all $ \gamma \in \Gamma(U) $ over $ W = U $ if $ |\Gamma^{-1}(\gamma)| \geq 2 $ for all $ \gamma \in \Gamma $.} \\

\textbf{Proof} \; Take $ U := \N $ and $ W = U $. Choose any $ \Gamma $ as described in the claim. Enumerate $ \gamma \in \Gamma(U) $ as $ 1, 2, \dots, i, \dots $ . Define $ V^{i} = \{u : \Gamma^{-1}(\gamma) = u \} $ for each $ i \in \Gamma(U) $. Similarly enumerate each $ u \in V^{i} $ as $ i_1, i_2, \dots, i_j, \dots $ . Now define the device $ (X, Y) $ as 

\begin{equation*}
X(i_j) = \begin{cases}
       a_i & \text{ if } j = 1 \\
       b_i & \text{ if } j = 2 \\
       c_i & \text{ otherwise } \\
       \end{cases} \quad \quad 
Y(i_j) = \begin{cases}
       1 & \text{ if } j = 1 \\
       -1 & \text{ otherwise. } \\
       \end{cases} 
\end{equation*}

Define $ \xi_{\gamma} : \Gamma(U) \rightarrow \overline{X} $ for each $ i \in \Gamma(U) $  as

\begin{equation*}
\xi_{\gamma}(i) = \begin{cases}
       \overline{a_i} & \text{ if } i = \gamma \\
       \overline{b_i} & \text{ otherwise. } \\
       \end{cases} 
\end{equation*}

Then for all $ \gamma \in \Gamma(U) $ and all $ u \in \xi_{\gamma}(\gamma')$, $ \delta_{\gamma}(\Gamma(u)) = Y(u) $. Furthermore, for each $ \xi_{\gamma} $, $ \emptyset \neq \{ ( \xi_{\gamma}(\gamma' = \gamma) \cap W) \subseteq Y^{-1}(1) \} $ and $ \emptyset \neq \{ ( \xi_{\gamma}(\gamma' \neq \gamma) \cap  W ) \subseteq Y^{-1}(-1) \} $. Hence, $ \mathcal{D} $ physically knows $ \gamma \in \Gamma(U) $. \\

\textbf{Lemma} \; \textit{For any set of independently physically knowable characteristic functions $ A^{*} $ there is a device $ \mathcal{D} $ and $ \xi $ such that $ \mathcal{D} $ knows $ A \in A^{*} $ over a given subset $ W \subseteq U $ using $ \xi $. } \\

\textbf{Proof} \;  

%%%%%%%%%%%%%%%%%%%%%%%% 
Let $ U := \N $ and take $ W = U $. Construct 

\begin{equation*}
X(i_j) = \begin{cases}
       a_i & \text{ if } j = 1 \\
       b_i & \text{ otherwise } \\
       \end{cases} \quad \quad 
Y(i_j) = \begin{cases}
       1 & \text{ if } j = 1 \\
       -1 & \text{ otherwise. } \\
       \end{cases} 
\end{equation*}

%%%%%%%%%%%%%%%%%%%%%%%%

\bigskip
\textbf{Theorem} \; \textit{Let c be a positive integer. Let $ A^{*} $ be a countable set of inequivalent characteristic functions and $ \mathcal{D} $ be a device that physically knows all functions $ a \in A^{*} $. Every finite subset $ A $ of $ A^{*} $ with cardinality $ m $ has at least $ m(1 - m^{-c}) + 1 $ functions $ a $ such that $ \mathcal{K}_{\mu}(a;\mathcal{D}) \geq \log m - c $.} \\

\textbf{Proof} \; 

\bigskip
\textbf{Theorem} \; \textit{Let $ \Gamma $ be a knowable function with countable range and $ \mathcal{D} $ be a device that physically knows all $ \gamma \in \Gamma $ over $ U $. Every such $ \Gamma $ with $ |\Gamma(U)| = m $ has knowledge complexity of at least $ - m \ln \sum_{i = 1}^{m} {\log_{2}{m}} $.} \\ 

\textbf{Proof} \;
Let $ U := \N $ and take $ W = U $. Choose any convenient knowable $ \Gamma $ with a countable range such that $ |\Gamma(U)| = m $. Then construct $ \mathcal{D} $ and $ \xi_{ \gamma \in \Gamma(U) } $ as specified in the knowability lemma. Then the knowledge complexity of $ \Gamma $ with respect to $ \mathcal{D} $ using $ \xi_{\gamma \in \Gamma(U)} $ is

$$ \mathcal{K}_{\mu}(\Gamma ; \mathcal{D}) = \sum_{\gamma \in \Gamma(U)} \min_{\xi(\gamma) : \mathcal{D} \text{ knows } \Gamma = \gamma \text{ using } \xi} [\mathcal{M}_{\mu; \xi} (\gamma)]	$$

$$ = \sum_{\gamma \in \Gamma(U)}  \min_{\xi(\gamma) : \mathcal{D} \text{ knows } \Gamma = \gamma \text{ using } \xi} - \ln \sum_{u \in \xi_{\gamma}(\gamma \in \Gamma(U))} \ell(u) = - m \ln \sum_{i = 1}^{m} {\log_{2}{m}} $$ 


%%%%%%%%%%%%%%%%%%%%%%%% INCOMPRESSIBILITY RESULT





%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Future Work}

There are many related areas to IDs deserving of attention. The question of whether non-deterministic Turing machines may be strongly inferred was left unattended. Further richness may be found by establishing symmetries between results concerning Kolmogorov complexity and inference complexity. 


\newpage
\section{Acknowledgements}

Wolpert DH. \textit{Constraints on physical reality arising from a formalization of knowledge}. arXiv preprint arXiv:1711.03499v3 [physics.hist-ph] (2018). \\
Li M \& Vitányi P. \textit{An Introduction to Kolmogorov Complexity and Its Applications} (1st ed.). Springer-Verlag. (1993). \\
Arora S \& Barak B. \textit{Computational Complexity: A Modern Approach}. Cambridge University Press. (2009). \\ 

\end{document}

