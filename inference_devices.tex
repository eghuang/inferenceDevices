\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}          
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{csquotes}
\usepackage{setspace}\onehalfspacing
\usepackage[loose,nice]{units} %replace "nice" by "ugly" for units in upright fractions
\usepackage{graphicx}
\graphicspath{ {/Users/eghuang/math104/images/} }
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=R,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\title{
  Notes on Inference Devices \\
  \large Santa Fe Institute}
\author{Edward G. Huang}
\date{Summer 2018} 
 
 \newcommand{\R}{\mathbb{R}}
 \newcommand{\Q}{\mathbb{Q}}
 \newcommand{\Z}{\mathbb{Z}}
 \newcommand{\N}{\mathbb{N}}
 \newcommand{\B}{\mathbb{B}}
 \newcommand{\Prob}{\mathbb{P}}
 \newcommand{\E}{\mathbb{E}}
 \newcommand{\code}[1]{\texttt{#1}}
 \let\oldemptyset\emptyset
 \let\emptyset\varnothing
 
 \setlength{\parindent}{0pt} % no indent
 
\begin{document}
\maketitle 

% SECTION 1: NOTATION
\section{Notation and Definitions} 

This manuscript utilizes standard notation taken from set theory and vector algebra. We clarify notation specific to Turing Machine theory and inference devices. \\


\subsection{Turing Machines} 

$ \B^{*} \quad $ The space of all finite bit strings. \\
$ \Lambda \quad $ Symbol alphabet of a Turing Machine. \\
$ \sigma \quad $ A symbol on a Turing Machine tape. \\
$ Q \quad $ Set of finite states of a Turing Machine. \\
$ \Delta \quad $ Transition function of a Turing Machine. \\
$ k \quad $ Number of tapes of a Turing Machine. The first tape is assumed to be read-only. \\
$ \eta \quad $ Non-halting state of a Turing Machine. \\

\subsection{Inference Devices} 
$ U \quad $ Set of possible histories of the universe. \\
$ u \quad $ A history of the universe in $ U $. \\ 
$ X \quad $ Setup function of an ID that maps $ U \rightarrow X(U) $. A binary question concerning $ \Gamma(u) $. \\
$ x \quad $ A binary question and a member of image $ X(U) $. \\ 
$ Y \quad $ Single-valued conclusion function of an ID that maps $ U \rightarrow \{-1, 1\} $. A binary answer of an ID for  $ X(u) = x $. \\ 
$ y \quad $ A single-valued answer, and member of image $ Y(U)  = \{0, 1\} $. \\ 
$ \Gamma \quad $ A function of the actual values of a physical variable over $U$, equivalent to $\Gamma(u) = S(t_i)(u)$.  \\
$ \gamma \quad $ Possible value of a physical variable, a member of the image $\Gamma(U)$. \\
$ \delta \quad $ Probe of any variable $V$ parameterized by $v \in V$ such that : 
	  \[ \delta_v (v') =
	  \begin{cases} 
       1 & \text{ if } v = v' \\
       -1 & \text{ otherwise } \\
      \end{cases}\] \\
$ \wp \quad $ Set of probes over $\Gamma(U)$. \\
$ \mathcal{D} = (X, Y) \quad $ An inference device, consisting of functions $ X $ and $ Y $. \\
$ \quad \bar{F} \quad $ Inverse. Given a function $ F $ over $ U $, $F ^ {-1} = \bar{F} \equiv \{\{u : F(u) = f \} : f \in F(U) \} $. \\
$ > \quad $ Weak inference: a device $\mathcal{D}$ weakly infers $\Gamma$ \textit{iff} $ \forall \gamma \in \Gamma(U), \exists x \in X(U) $ s.t. $ \forall u \in U $, 

$ \quad \quad X(u) = x \implies Y(u) = \delta_{\gamma}(\Gamma(u)) $.  \\
$ \gg \quad $ Strong inference: a device $ (X, Y) $ strongly infers a functions $ (S, T) $ over $ U $ \textit{ iff } $\forall \delta \in \wp(T) $ 

\quad \quad and all $ s \in S(U) $, $ \exists x $ \textit{ such that } $ X(u) = x \implies S(u) = s, Y(u) = \delta(T(u)) $. \\

% SECTION 2: TURING MACHINES
\section{Turing Machines} 

\subsection{Deterministic Turing Machines} 

 Arora and Barak denote a Turing Machine (TM) as $ T = (\Lambda, Q, \Delta) $ containing:
\begin{enumerate}
\item An \textit{alphabet} $ \Lambda $ of a finite set of symbols that $ T $'s tapes can contain. We assume that $ \Lambda $ contains a special blank symbol $ B $, start symbol $ S $, and the symbols 0 and 1. 
\item A finite set $ Q $ of possible states that $ T $'s register can be in. We assume that $ Q $ contains a special start state $ q_{s} $ and a special halt state $ q_{h} $. 
\item A transition function function $ \Delta : Q \times \Lambda^{k} \rightarrow Q \times \Lambda^{k - 1} \times \{L, \mathcal{S}, R\}^{k} $, where $ k \geq 2$, describing the rules $ T $ use in performing each step. The set $\{L, \mathcal{S}, R\}$ denote the actions \textit{Left, Stay,} and \textit{Right}, respectively. 
\end{enumerate}

Suppose $ T $ is in state $ q \in Q $ and $ (\sigma_1, \sigma_2, \dots, \sigma_k) $ are the symbols on the $ k $ tapes. Then $ \Delta(q, (\sigma_1, \dots, \sigma_k)) = (q', (\sigma_{2}^{'}, \dots, \sigma_{k}^{'}), z) $ where $ z \in \{L, \mathcal{S}, R\}^k $ and at the next step the $ \sigma $ symbols in the last $ k - 1 $ tapes will be replaced by the $ \sigma' $ symbols, the machine will be in state $ q $, and the $ k $ heads will move \textit{Left, Right} or \textit{Stay}. This is illustrated in Figure 2.1. \\

\textbf{Figure 2.1. The transition function $ \Delta $ for a $ k $-tape Turing Machine}

 \begin{center}
 \begin{tabular}{ c|c|c|c||c|c|c|c|c } 
 \hline
 \multicolumn{4}{c||}{ $ (q, (\sigma_1, \dots, \sigma_k)) $ } & 
      \multicolumn{5}{c}{ $ (q', (\sigma_{2}^{'}, \dots, \sigma_{k}^{'}), z) $ } \\
 
 \hline
 \begin{tabular}[c]{@{}c@{}} Input \\ symbol \end{tabular} & 
 \begin{tabular}[c]{@{}c@{}} Work/output \\ symbol \\ read \end{tabular} & 
 $\dots $ &
 \begin{tabular}[c]{@{}c@{}} Current \\ state \end{tabular} &
 \begin{tabular}[c]{@{}c@{}} New \\ work/output \\ tape symbol \end{tabular} & 
 $ \dots $ &
 \begin{tabular}[c]{@{}c@{}} Move \\ work/output \\ tape \end{tabular} &
 $ \dots $ &
 \begin{tabular}[c]{@{}c@{}} New \\ state \end{tabular} \\ 
 
 \hline
 \hline
 $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ \\ 
 \hline
 $  \sigma_1 $ & $ \sigma_i $ & $ \ddots $ & $ q $ & $ \sigma_i^{'} $ & $ \ddots $ & $ z_i $ & $ \ddots $ & $ q^{'} $ \\ 
 \hline
 $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ \\ 
 \end{tabular}
 \end{center} 

\bigbreak

\textit{Remark}: $\Lambda$ can be reduced to $ \B = \{0, 1\} $ and $ k $ can be reduced to $ 1 $ without loss of computational power. Then, any Turing Machine can be expressed as a partial recursive function mapping $ \B^{*} \rightarrow \B^{*} \cup \eta $, where $ \eta $ is the undefined non-halting output. Since $ |\B^{*} \times \B^{*} \cup \eta | = | \N \times \N | = | \N | $, the set of all Turing Machines is countably infinite. 

\subsection{Non-deterministic Turing Machines} 

Non-deterministic Turing Machines (NDTM) differ from deterministic Turing Machines by having two transition functions $ \Delta_0, \Delta_1 $ and a special state $ q_{accept} $. From Arora and Barak:

\begin{displayquote}
When a NDTM $M$ computes a function, we envision that at each computational step $ M $ makes an arbitrary choice as to which of its two transition functions to apply. For every input $ x $, we say that $ M(x) = 1 $ if there exists some sequence of these choices (which we call nondeterministic choices of $ M $) that would make $ M $ reach $ q_{accept} $ on input $ x $. Otherwise - if every sequence of choices makes $ M $ halt without reaching $ q_{accept} $ - then we say that $ M(x) = 0 $. 
\end{displayquote}

If $ M(x) = 1$, we say that $ M $ accepts the input $ x $. There are two ways to interpret the choice of update function to use in a NDTM. We can either assume that the NDTM chooses updates that will lead to an accepting state, or we can assume that the machine branches out into its choices such that it has a "computation tree" and if any of the branches reaches the accepting state then the machine accepts the input. From this second interpretation, the computational power of DTMs to NDTMs is analogous to the computational complexity of P to NP. \\

% SECTION 3: STRONG INFERENCE
\section{Strong Inference} 
 
In the next three examples we examine strong inference of integer-valued functions. \\
 
 
 \textbf{Example 3.1} \quad Let $ T(U) = \{0 , 1\} $ and $ S(U) = \{0, 1, 2\} $. We construct $ (X, Y) $ in the table at the left such that it strongly infers $ (S, T) $. The right table indicates $ x $ for each $ s $, $ \delta $ such that the definition of strong inference is satisfied: \\ 
 \begin {center}
 \begin{tabular}{ |c||c|c|c|c| } 

 \hline
 $ u $ & $ X(u) $ & $ Y(u) $ & $ S(u) $ & $ T(u) $ \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ 1 $ & $ 0 $ & $ 0 $ \\
 \hline
 $ 2 $ & $ 2 $ & $ -1 $ & $ 0 $ & $ 0 $ \\
 \hline
 $ 3 $ & $ 3 $ & $ 1 $ & $ 1 $ & $ 0 $ \\
 \hline
 $ 4 $ & $ 4 $ & $ -1 $ & $ 1 $ & $ 0 $ \\
 \hline 
 $ 5 $ & $ 5 $ & $ 1 $ & $ 2 $ & $ 1 $ \\
 \hline 
 $ 6 $ & $ 6 $ & $ -1 $ & $ 2 $ & $ 1 $ \\
 \hline
 \end{tabular} 
 \quad 
 \begin{tabular}{ |c||c|c| } 

 \hline
 $ s $ \textbackslash $ \delta $ & $ \delta_0 $ & $ \delta_1 $ \\ 
 \hline
 \hline
 $ 0 $ & $ 1 $ & $ 2 $  \\
 \hline
 $ 1 $ & $ 3 $ & $ 4 $ \\
 \hline
 $ 2 $ & $ 6 $ & $ 5 $ \\
 \hline
 
 \end{tabular}
 \end{center}
 
  
 \bigskip
 \bigskip 
 \textbf{Example 3.2} \quad Let $ T(U) = \{1, 2, 3\} $ and $ S(U) = \{1, 2, 3, 4, 5\} $. Again, we construct $ (X, Y) $ in the table at the left such that it strongly infers $ (S, T) $. The right table indicates $ x $ for each $ s $, $ \delta $ such that the definition of strong inference is satisfied: \\ 

 \begin{center}
 \begin{tabular}{ |c||c|c|c|c| } 

 \hline
 $ u $ & $ X(u) $ & $ Y(u) $ & $ S(u) $ & $ T(u) $ \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ 1 $ & $ 1 $ & $ 1 $ \\
 \hline
 $ 2 $ & $ 2 $ & $ -1 $ & $ 1 $ & $ 1 $ \\
 \hline
 $ 3 $ & $ 3 $ & $ 1 $ & $ 2 $ & $ 1 $ \\
 \hline
 $ 4 $ & $ 4 $ & $ -1 $ & $ 2 $ & $ 1 $ \\
 \hline
 $ 5 $ & $ 5 $ & $ 1 $ & $ 3 $ & $ 2 $ \\
 \hline
 $ 6 $ & $ 6 $ & $ -1 $ & $ 3 $ & $ 2 $ \\
 \hline
 $ 7 $ & $ 7 $ & $ 1 $ & $ 4 $ & $ 2 $ \\
 \hline
 $ 8 $ & $ 8 $ & $ -1 $ & $ 4 $ & $ 2 $ \\
 \hline
 $ 9 $ & $ 9 $ & $ 1 $ & $ 5 $ & $ 3 $ \\
 \hline
 $ 10 $ & $ 10 $ & $ -1 $ & $ 5 $ & $ 3 $ \\
 \hline 
 \end{tabular} 
 \quad
 \begin{tabular}{ |c||c|c|c| } 

 \hline
 $ s $ \textbackslash $ \delta $ & $ \delta_1 $ & $ \delta_2 $ & $ \delta_3 $  \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ 2 $ & $ 2 $ \\
 \hline
 $ 2 $ & $ 3 $ & $ 4 $ & $ 4 $ \\
 \hline
 $ 3 $ & $ 6 $ & $ 5 $ & $ 6 $ \\
 \hline
 $ 4 $ & $ 8 $ & $ 7 $ & $ 8 $ \\
 \hline 
 $ 5 $ & $ 10 $ & $ 10 $ & $ 9 $ \\
 \hline
 
 \end{tabular}
 \end{center} 

 \bigskip
 \bigskip 
 \textbf{Example 3.3} \quad Let $ T(U) = \{1, 2, 3\} $ and $ S(U) = \{1, 2\} $. In this example, the inferred function $ f: S \rightarrow T, f(s) = T(S^{-1}(s)) $ is not single-valued. \\ 

 \begin{center}
 \begin{tabular}{ |c||c|c|c|c| } 

 \hline
 $ u $ & $ X(u) $ & $ Y(u) $ & $ S(u) $ & $ T(u) $ \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ -1 $ & $ 1 $ & $ 1 $ \\
 \hline
 $ 2 $ & $ 2 $ & $ -1 $ & $ 1 $ & $ 2 $ \\
 \hline
 $ 3 $ & $ 3 $ & $ -1 $ & $ 1 $ & $ 3 $ \\
 \hline
 $ 4 $ & $ 4 $ & $ -1 $ & $ 2 $ & $ 1 $ \\
 \hline
 $ 5 $ & $ 5 $ & $ -1 $ & $ 2 $ & $ 2 $ \\
 \hline
 \end{tabular} 
 \quad
 \begin{tabular}{ |c||c|c|c| } 

 \hline
 $ s $ \textbackslash $ \delta $ & $ \delta_1 $ & $ \delta_2 $ & $ \delta_3 $ \\ 
 \hline
 \hline
 $ 1 $ & $ 2 $ & $ 1 $ & $ 1 $  \\
  \hline
 $ 2 $ & $ 5 $ & $ 4 $ & $ 4 $  \\
 \hline
 
 \end{tabular}
 \end{center} 

\bigskip 
\bigskip
% SECTION 4: INFERENCE OF TURING MACHINES
\section{Inference of Turing Machines}
\textbf{Theorem} \quad \textit{A deterministic Turing Machine} $(\Lambda, Q, \Delta)$ \textit{can be strongly inferred by a device if} 
$$\forall s \in S(U),\; |S^{-1}(s)| \geq 2. $$ \textit{This holds for both the representation of a Turing Machine as a partial recursive function and the representation as an update function.} \\

\textbf{Proof} \quad We first examine the partial function case. Let $ f $ be the partial recursive function that describes the given Turing Machine tuple. Let $ U := \N $. Choose any convenient single valued surjective function $ S: U \rightarrow \B^{*} $ and define $ T: U \rightarrow \B^{*} \cup \eta$ by $T(u) = f(S(u)) $ as the single-valued function mapping $ U $ to the halting and non-halting outputs of $ f $. Then $ f $ can be written as the single-valued mapping $ S \rightarrow T $ by $ f(s) = T(S^{-1}(s)) $. \\

% delta and f must be in definitions of s and t.

Enumerate the elements of $ S(U) $ as $ 1, 2, \dots, s, \dots \; $. Let $ V^s = \{u : S^{-1}(s) = u \} $ for $ s \in S(U) $. Similarly enumerate the elements of $ V^s $ as $ s_1, s_2, \dots , s_{|V^{s}|}$. Then define $ X $ and $ Y $ as follows:

\begin{equation*}
X(s_i) = \begin{cases}
       a_s & \text{ if } i = 1 \\
       b_s & \text{ otherwise } \\
       \end{cases} \quad \quad 
Y(s_i) = \begin{cases}
       1 & \text{ if } i = 1 \\
       -1 & \text{ otherwise } \\
       \end{cases} 
\end{equation*}

\bigskip
Note that the condition $ |S^{-1}(s)| \geq 2 $ is required to guarantee $ Y(V^s) = \{1, -1\} $. For each pair $ (s, \delta_{t \in T(U)} ) $, to force $ S(u) = s $ and $ Y(u) = \delta_{t}(T(u)) $, choose $ x = a_s $ if $ t = T(s_1) $ or otherwise choose $ x = b_s $. Since the choices of $ s $ and $ t $ were arbitrary, this holds for all $ (s, \delta_t) $ pairs. \\

Now consider the update function that describes the given Turing Machine. Recall that the update function is written as $ \Delta: Q \times \Lambda^{k} \rightarrow Q \times \Lambda^{k - 1} \times \{ L, \mathcal{S}, R \} ^{k} $, $ k \geq 2 $. Consider a convenient single-valued surjective function $ S: U \rightarrow Q \times \Lambda^{k} $ representing the possible inputs for a Turing Machine and a corresponding single-valued $ T: U \rightarrow Q \times \Lambda^{k - 1} \times \{ L, \mathcal{S}, R \}^ {k} $ as $ T(u) = \Delta(S(u)) $. Then $ \Delta $ can be written as the single-valued function $ \Delta(s) = T(S^{-1}(s)). $ \\

We proceed analogously to the partial function portion of the proof. Define $ V^s $, $ X $, and $ Y $ as described in the preceding paragraphs. For each pair $ (s, \delta_{t \in T(U)} ) $, to force $ S(u) = s $ and $ Y(u) = \delta_{t}(T(u)) $, choose $ x = a_s $ if $ t = T(s_1) $ or otherwise choose $ x = b_s $. Since the choices of $ s $ and $ t $ were arbitrary, this holds for all $ (s, \delta_t) $ pairs. $ \hfill \qed $ \\

\textit{Remark:} As a point of convention, all functions over $ U $ must have a range of at least two elements. This implies that Turing machines that never halt cannot be strongly inferred.

% NECESSARY <- PROOF, DOES NOT CONSIDER CASE WHERE TURING MACHINE NEVER HALTS
%Now for either case, suppose that $|V| < 2$ for some $ s $. If $ V = \emptyset $ then there exists no $ x $ that can force $ S = s $. If $ |V| = 1 $, then we can assign $ Y(v) = y \in \{-1, 1\} $. However, whatever value we assign, we cannot guarantee that $ \delta_{t}(T(v)) = Y(v) $ for all $ t $ since $ |T(U)| > 1 $. $ \hfill \qed $ \\


%\bigskip
%\bigskip
%\textbf{Corollary} \quad \textit{}



%\bigskip
%\bigskip
%\textbf {Theorem} \quad \textit{Any non-deterministic Turing Machine can be strongly inferred by a device if} 
%$$\forall s \in S(U),\; |S^{-1}(s)| \geq 2. $$
%\textit{This condition holds for both the partial function representation and the instantaneous description of the Turing Machines.} \\

% # NOTES #
% Don't need to consider partial function case
% Need to construct two update functions 
%Consider the two update functions $ \Delta_0 $, $ \Delta_1 : Q \times \Lambda^{k} \rightarrow Q \times \Lambda^{k - 1} \times \{ L, S, R \} ^{k} $, $ k \geq 2 $ of a nondeterministic Turing machine. Let $ U := \N $. Choose any convenient single-valued surjective function $ S: U \rightarrow Q \times \Lambda^{k} $ and a corresponding single-valued $ T: U \rightarrow Q \times \Lambda^{k - 1} \times \{ L, S, R \}^ {k} $ as:
%
%\begin{equation*}
% T(u) = \begin{cases}
%        \Delta_0(S(u)) \\
%        \Delta_1(S(u)) \\
%        \end{cases} \text{arbitrarily}
%\end{equation*} 
%       
%Then we write $ \Delta_{0}(s), \Delta_{1}(s) = T(S^{-1}(s)) $


% Does this mean that there can be different t for each u? 

% #########



% fix x and loop s, t

% Encode not the update rule but the entire map 

% Use countable spaces
% Gamma has to encode input string, output string or nonhalting 

% Does strong inference solve the Halting Problem? 

% Go through all the counting numbers, map from counting numbers to joint space (b, T_1(b))
% T(u) is a value in the space of strings union no halt such that T(u) is the associated output of S(u). 

% Use tuple to define Gamma, probe cares about second part 


\section{Inference Complexity}

\textbf{Definition} \quad Let $ \mathcal{D} $ be an inference device and $ \Gamma $ be a function over $ U $ where $ X(U) $ and $ \Gamma(U) $ are countable and $ \mathcal{D} > \Gamma $. Let the \textbf{size} of $\gamma \in \Gamma(U) $ be written as $ \mathcal{M}_{\mu:\Gamma(\gamma)} = -\ln[\int_{\Gamma^{-1}(\gamma)} d\mu(u) 1] $. Then the \textbf{inference complexity} of $ \Gamma $ with respect to $ \mathcal{D} $ and measure $ \mu $ is defined as: 

$$ \mathcal{C}_{\mu}(\Gamma ; \mathcal{D}) \triangleq \sum_{\delta \in \wp(\Gamma)} min_{x : X = x \implies Y = \delta(\Gamma) } [\mathcal{M}_{\mu, X} (x)]	$$ 

\textit{Remark:} This is only a working definition and may be revised depending on its behavior.

% Make strong inference example for arxiv paper
% use a tuple for x <- (b, probe type)

\end{document}