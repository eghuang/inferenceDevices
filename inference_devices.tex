\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}          
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{csquotes}
\usepackage{setspace}\onehalfspacing
\usepackage[loose,nice]{units} %replace "nice" by "ugly" for units in upright fractions
\usepackage{graphicx}
\usepackage{listings}
\usepackage{color}
\usepackage{glossaries}	
\usepackage{enumitem}% http://ctan.org/pkg/enumitem
\usepackage[center, md]{titlesec} % add ", md" behind center to remove bolding


\graphicspath{ {/Users/eghuang/math104/images/} }

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=R,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\title{
  Algorithmic information and inference devices }
\author{Edward G. Huang \\ {\small Santa Fe Institute } \\ {\small Department of Mathematics, University of California, Berkeley } \\ \lstinline{eghuang@berkeley.edu}}
\date{ {\normalsize August 2018} }

 \newcommand{\R}{\mathbb{R}}
 \newcommand{\Q}{\mathbb{Q}}
 \newcommand{\Z}{\mathbb{Z}}
 \newcommand{\N}{\mathbb{N}}
 \newcommand{\B}{\mathbb{B}}
 \newcommand{\Prob}{\mathbb{P}}
 \newcommand{\E}{\mathbb{E}}
 \newcommand{\code}[1]{\texttt{#1}}
 \let\oldemptyset\emptyset
 \let\emptyset\varnothing
 \setlength{\parindent}{0pt} % no indent
 
\begin{document}
\maketitle 
% HOOPER: Please include an abstract, a brief intro with a clear statement of your research question(s), a description of your methods/model/approach, your results, and your conclusions (or lessons learned).
% SECTION 0: ABSTRACT
\renewcommand{\abstractname}{\vspace{-\baselineskip}}
\begin{abstract}
There has been much interest surrounding what properties about the universe can be derived from applying a mathematical formalization of inference and knowledge. Previously, Wolpert demonstrated bounds on knowledge in any physical universe that allows agents to hold information concerning that universe by using the concept of "inference devices" (IDs). We extend previous work on the capacity and limitations of IDs to infer physical variables. We then explore analogues between IDs and their relation to Turing machine theory and algorithmic information theory.
\end{abstract}

% SECTION 1: INTRODUCTION
\section{Introduction} 

% SUBSECTION: NOTATION
\subsection{Notation and Definitions}
This manuscript utilizes standard notation taken from set theory and vector algebra. We clarify notation specific to Turing machine theory and inference devices. 

\subsubsection{Turing Machine Notation} 
\begin{tabular}{cp{0.6\textwidth}}
$ \B^{*} $ & The space of all finite bit strings, $ \{\epsilon, 0, 1, 00, 01, \dots \} $\\
$ \Lambda $ & Symbol alphabet of a Turing Machine. \\
$ \sigma  $ & A symbol on a Turing Machine tape. \\
$ Q $ & Set of finite states of a Turing Machine. \\
$ \Delta $ & Transition function of a Turing Machine. \\
$ k $ & Number of tapes of a Turing Machine. The first tape is assumed to be read-only. \\
$ \eta $ & Non-halting state of a Turing Machine. \\
\end{tabular}

\subsubsection{Inference Device Notation} 
\begin{tabular}{cp{0.6\textwidth}}
$ U $ & Set of possible histories of the universe. \\
$ u $ & A history of the universe in $ U $. \\ 
$ X $ & Setup function of an ID that maps $ U \rightarrow X(U) $. A binary question concerning $ \Gamma(u) $. \\
$ x $ & A binary question and a member of image $ X(U) $. \\ 
$ Y $ & Single-valued conclusion function of an ID that maps $ U \rightarrow \{-1, 1\} $. A binary answer of an ID for  $ X(u) = x $. \\ 
$ y $ & A single-valued answer, and member of image $ Y(U)  = \{0, 1\} $. \\ 
$ \Gamma $ & A function of the actual values of a physical variable over $U$, equivalent to $\Gamma(u) = S(t_i)(u)$.  \\
$ \gamma $ & Possible value of a physical variable, a member of the image $\Gamma(U)$. \\
$ \delta $ & Probe of any variable $V$ parameterized by $v \in V$ such that : 
	  \[ \delta_v (v') =
	  \begin{cases} 
       1 & \text{ if } v = v' \\
       -1 & \text{ otherwise } \\
      \end{cases}\] \\
$ \wp $ & Set of probes over $\Gamma(U)$. \\
$ \mathcal{D}  $ & An inference device, consisting of functions $ (X, Y) $. \\
$ \xi $ & A function $ \Gamma(U) \rightarrow \overline{X} $.  \\
$ \Gamma^{-1} $ & Inverse. Given a function $ \Gamma $ over $ U $, $\Gamma ^ {-1} = \bar{\Gamma} \equiv \{\{u : \Gamma(u) = \gamma \} : \gamma \in \Gamma(U) \} $. \\
$ \overline{\Gamma} $ & Given a function $ \Gamma $ over $ U $, the partition of $ U $ given by $ \Gamma^{-1} $.  \\
$ > $ & Weak inference: a device $\mathcal{D}$ weakly infers $\Gamma$ \textit{iff} $ \forall \gamma \in \Gamma(U), \exists x \in X(U) $ s.t. $ \forall u \in U $, 
$ X(u) = x \implies Y(u) = \delta_{\gamma}(\Gamma(u)) $.  \\
$ \gg $ & Strong inference: a device $ (X, Y) $ strongly infers a function $ (S, T) $ over $ U $ \textit{ iff } $\forall \delta \in \wp(T) $ and all $ s \in S(U) $, $ \exists x $ \textit{ such that } $ X(u) = x \implies S(u) = s, Y(u) = \delta(T(u)) $. \\
$ \mathcal{C}_{\mu}(\Gamma; \mathcal{D}) $ & Inference complexity. \\
$ \mu $ & Measure defined for $ u \in U $. \\

\end{tabular}

% SUBSECTION: INFERENCE DEVICE
\subsection{Inference Devices}

% SUBSECTION: TURING MACHINES
\subsection{Turing Machines} 

% SUBSECTION: DTM
\subsubsection{Deterministic Turing Machines} 
 Arora and Barak denote a Turing Machine (TM) as $ T = (\Lambda, Q, \Delta) $ containing:
\begin{enumerate}
\item An \textit{alphabet} $ \Lambda $ of a finite set of symbols that $ T $'s tapes can contain. We assume that $ \Lambda $ contains a special blank symbol $ B $, start symbol $ S $, and the symbols 0 and 1. 
\item A finite set $ Q $ of possible states that $ T $'s register can be in. We assume that $ Q $ contains a special start state $ q_{s} $ and a special halt state $ q_{h} $. 
\item A transition function $ \Delta : Q \times \Lambda^{k} \rightarrow Q \times \Lambda^{k - 1} \times \{L, \mathcal{S}, R\}^{k} $, where $ k \geq 2$, describing the rules $ T $ use in performing each step. The set $\{L, \mathcal{S}, R\}$ denote the actions \textit{Left, Stay,} and \textit{Right}, respectively. 
\end{enumerate}

Suppose $ T $ is in state $ q \in Q $ and $ (\sigma_1, \sigma_2, \dots, \sigma_k) $ are the symbols on the $ k $ tapes. Then $ \Delta(q, (\sigma_1, \dots, \sigma_k)) = (q', (\sigma_{2}^{'}, \dots, \sigma_{k}^{'}), z) $ where $ z \in \{L, \mathcal{S}, R\}^k $ and at the next step the $ \sigma $ symbols in the last $ k - 1 $ tapes will be replaced by the $ \sigma' $ symbols, the machine will be in state $ q $, and the $ k $ heads will move \textit{Left, Right} or \textit{Stay}. This is illustrated in the following figure. \\

\textsc{Figure.} \; The transition function $ \Delta $ for a $ k $-tape Turing Machine

 \begin{center}
 \begin{tabular}{ c|c|c|c||c|c|c|c|c } 
 \hline
 \multicolumn{4}{c||}{ $ (q, (\sigma_1, \dots, \sigma_k)) $ } & 
      \multicolumn{5}{c}{ $ (q', (\sigma_{2}^{'}, \dots, \sigma_{k}^{'}), z) $ } \\
 
 \hline
 \begin{tabular}[c]{@{}c@{}} Input \\ symbol \end{tabular} & 
 \begin{tabular}[c]{@{}c@{}} Work/output \\ symbol \\ read \end{tabular} & 
 $\dots $ &
 \begin{tabular}[c]{@{}c@{}} Current \\ state \end{tabular} &
 \begin{tabular}[c]{@{}c@{}} New \\ work/output \\ tape symbol \end{tabular} & 
 $ \dots $ &
 \begin{tabular}[c]{@{}c@{}} Move \\ work/output \\ tape \end{tabular} &
 $ \dots $ &
 \begin{tabular}[c]{@{}c@{}} New \\ state \end{tabular} \\ 
 
 \hline
 \hline
 $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ \\ 
 \hline
 $  \sigma_1 $ & $ \sigma_i $ & $ \ddots $ & $ q $ & $ \sigma_i^{'} $ & $ \ddots $ & $ z_i $ & $ \ddots $ & $ q^{'} $ \\ 
 \hline
 $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ \\ 
 \end{tabular}
 \end{center} 

\bigbreak

\textsc{Remark.} $\Lambda$ can be reduced to $ \B = \{0, 1\} $ and $ k $ can be reduced to $ 1 $ without loss of computational power. Then, any Turing Machine can be expressed as a partial recursive function mapping $ \B^{*} \rightarrow \B^{*} \cup \eta $, where $ \eta $ is the undefined non-halting output. Since $ |\B^{*} \times \B^{*} \cup \eta | = | \N \times \N | = | \N | $, the set of all Turing Machines is countably infinite. \\

% SUBSECTION: NDTM
\subsubsection{Non-deterministic Turing Machines} 
Non-deterministic Turing Machines (NDTM) differ from deterministic Turing Machines by having two transition functions $ \Delta_0, \Delta_1 $ and a special state $ q_{accept} $. From Arora and Barak:

\begin{displayquote}
When a NDTM $M$ computes a function, we envision that at each computational step $ M $ makes an arbitrary choice as to which of its two transition functions to apply. For every input $ x $, we say that $ M(x) = 1 $ if there exists some sequence of these choices (which we call nondeterministic choices of $ M $) that would make $ M $ reach $ q_{accept} $ on input $ x $. Otherwise - if every sequence of choices makes $ M $ halt without reaching $ q_{accept} $ - then we say that $ M(x) = 0 $. 
\end{displayquote}

If $ M(x) = 1$, we say that $ M $ accepts the input $ x $. There are two ways to interpret the choice of update function to use in a NDTM. We can either assume that the NDTM chooses updates that will lead to an accepting state, or we can assume that the machine branches out into its choices such that it has a "computation tree" and if any of the branches reaches the accepting state then the machine accepts the input. From this second interpretation, the computational power of DTMs to NDTMs is analogous to the computational complexity of P to NP. \\

% SUBSECTION: UTM
\subsubsection{Universal Turing Machines}

% SECTION: WEAK INFERENCE
\newpage
\section{Weak Inference}
\bigskip   

\textsc{Definition} \quad Two functions $ \Gamma_1 $ and $ \Gamma_2 $ with the same domain $ U $ are \textbf{(functionally) equivalent} iff the inverse functions $ \Gamma^{-1}_1 $ and $ \Gamma^{-1}_2 $ induce the same partitions of $ U $, i.e., iff $ \overline{\Gamma_1} = \overline{\Gamma_2} $. \\

% LEMMA 1

\textsc{Lemma} \; \textit{A function} $ \Gamma $ \textit{can be weakly inferred by a device} $ \mathcal{D} $ \textit{if} $ |\Gamma^{-1}(\gamma)| \geq 2 $ \textit{for any} $ \gamma \in \Gamma(U) $. \\
\textsc{Proof} \; Let $ U := \N $. Enumerate $ \gamma \in \Gamma $ as $ 1, 2, \dots, i, \dots, n $ and define $ V^{i} = \{u: \Gamma^{-1}(i) = u \} $. Then enumerate each element in $ V^i $ as $ i_1, i_2, \dots, i_j, \dots, i_m $. Continue to define $ X $ and $ Y $ as 

\begin{equation*}
X(i_j) = \begin{cases}
       a_i & \text{ if } j = 2 \\
       b_i & \text{ otherwise } \\
       \end{cases} \quad \quad 
Y(i_j) = \begin{cases}
       -1 & \text{ if } j = 2 \\
       1 & \text{ otherwise. } \\
       \end{cases} 
\end{equation*}

Then for each $ i \in \Gamma(U) $ choose $ x = b_i $ to force $ Y(X^{-1}(b_i)) = \delta_{i}(\Gamma(X^{-1}(b_i)) = i) = 1. \hfill \qed $ \\

% LEMMA 2
\bigskip   
\textsc{Lemma} \; \textit{A function} $ \Gamma $ \textit{can be weakly inferred by a device} $ \mathcal{D} $ \textit{if} $ |\Gamma(U)| \geq 3 $.  \\
\textsc{Proof} \; Let $ U := \N $. Enumerate $ \gamma \in \Gamma(U) $ as $ 1, 2, \dots, i, \dots, n $ and define $ V^{i} = \{u: \Gamma^{-1}(i) = u \} $. Then enumerate each element in $ V^i $ as $ i_1, i_2, \dots, i_j, \dots, i_m $. Continue to define $ X $ and $ Y $ as 

\begin{equation*}
X(i_j) = \begin{cases}
       a & \text{ if } i = 1 \text{ and } j = 1 \\
       b & \text{ if } i = 2 \text{ and } j = 1 \\
       c & \text{ otherwise. } \\
       \end{cases} \quad \quad 
Y(i_j) = \begin{cases}
       1 & \text{ if } i = 1 \text{ and } j = 1 \\
       -1 & \text{ otherwise. } \\
       \end{cases} 
\end{equation*}

Then for each $ i \in \Gamma(U) $ to force $ Y(X^{-1}(x)) = \delta_{i}(\Gamma(X^{-1}(x)) = i) $ choose $ x = a $ if $ i = 1 $, $ x = c $ if $ i = 2 $, or else choose $ x = b $. $ \hfill \qed $ \\

% COROLLARY 2
\bigskip
\textsc{Corollary} \; \textit{A function $\Gamma$ cannot be weakly inferred by ay device if $ |\Gamma{-1}(\gamma)| <  2 $ and $ |\Gamma(U)| < 3 $}. \\
\textsc{Proof} \; Fix any such $ \Gamma $ and set $ U = \{1, 2\} $. Let $ \Gamma(1) = \gamma_1 $, $\Gamma(2) = \gamma_2 $. Then either $ Y(1) = 1, Y(2) = -1 $, or vice versa. For the first case, consider the probe $ \delta_{\gamma_{2}} $ to see that there is no $ x \in X(U) $ that forces $ Y(u) = \delta_{\gamma_{2}}(\Gamma(u)) $. Consider $ \delta_{\gamma_{1}} $ to see that weak inference does not hold for the remaining case.  \hfill \qed

% COUNTABLE INFERENCE THEOREM
\bigskip
\newpage
\textsc{Theorem} \; \textit{A countable set of inequivalent functions} $ A^{*} $ \textit{can be weakly inferred by a device if each function $ A_i \in A^{*} $ is independently inferable. } \\

\textsc{Proof} \; Let $ U := \N $ and fix any $ A^{*} $. Let $ a_i, a_j, a_k $ represent any distinct three elements in $ A_i(U) $. Write $ V = \{1, 2, 3\} \subset U $. The following table represents all possible combinations of $ a_i $, $ a_j$, and $ a_k $ over $ V $. \\
 \begin{center}
 \begin{tabular}{ |c||c|c|c|c|c|c|c|c } 

 \hline
 $ u $ & $ X(u) $ & $Y(u)$ & $ A_1(u) $ & $ A_2(u) $ & $ A_3(u) $ & $ A_4(u) $ & $ A_5(u) $ & $ \dots $ \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ -1 $ & $ a_i $ & $ a_i $ & $ a_i $ & $ a_i $ & $ a_i $ & $ \dots $ \\
 \hline
 $ 2 $ & $ 2 $ & $ -1 $ & $ a_i $ & $ a_j $ & $ a_j $ & $ a_j $ & $ a_i $ & $ \dots $ \\
 \hline
 $ 3 $ & $ 3 $ & $ 1 $ & $ a_i $ & $ a_k $ & $ a_j $ & $ a_i $ & $ a_j $ & $ \dots $ \\
 \hline
 $ \vdots $ & $ \vdots $ & $ \vdots $ & $ \vdots $ & $ \vdots $ & $ \vdots $ & $ \vdots $ & $ \vdots $ & $ \ddots $ \\
 \end{tabular} 
 \end{center}
\bigskip 
Note that setting $ X(u) = u $, $ Y(1) = -1 $, $ Y(2) = -1 $, and $ Y(3) = 1 $ immediately satisfies weak inference for any function $ A_i $ that is functionally equivalent to $ A_1 $, $ A_2 $, $ A_3 $ and $ A_4 $ over $ V $. This holds regardless of the values of $ Y(u) $ and $ A_{ i \in \{1, 2, 3, 4\} }(u) $ that follow for $ u > 3 $. This is shown by selecting $ x $ for the following cases:

\bigskip
\textit{Case 1:} $ \overline{A_i(V)} = \overline{A_1(V)} $. \\
 Choose $ x = 3 $ for $ a = a_i $ or otherwise choose $ x = 2 $. \\

\textit{Cases 2, 3, 4:} $ \overline{A_i(V)} = \overline{A_2(V)} $ or  $ \overline{A_3(V)} $ or $ \overline{A_4(V)} $. \\
 Choose $ x = 2 $ for $ a = a_i $ or otherwise choose $ x = 1 $. \\

Now we need to guarantee weak inference for $ A_i $ that are functionally equivalent to $ A_5 $ over $ V $. Enumerate each $ A_i \in A^{*}: \overline{A_i(V)} = \overline{A_5(V)} $ as $ B_4, B_5, \dots, B_i, \dots, B_n $. To satisfy weak inference for any $ A_i $ and $ B_i $, define $ X $, $ Y $ more explicitly as 
\begin{equation*} 
     X(u) = u, \quad \quad
     Y(u) = \begin{cases} 
    -1 & \text{ if } u = 1, 2 \text { or if } B_i(u) = a_j, a_k: X(u) = i \\
     1 & \text{ if } u = 3 \text{ or if } B_i(u) = a_i: X(u) = i \\
    -1 & \text { otherwise. } \\
            \end{cases} \\
\end{equation*}

For each $ a \in B_i(U) $ to force $ Y(X^{-1}(x)) = \delta_{a}(B(X^{-1}(x))) $ choose $ x = i $ if $ a = a_i $ or otherwise choose $ x = 1 $. \hfill \qed  \\


% SECTION 3: STRONG INFERENCE
\newpage
\section{Strong Inference} 
 
In the next three examples we examine strong inference of integer-valued functions. \\
 
% EXAMPLE 
 \textsc{Example.} \quad Let $ T(U) = \{0 , 1\} $ and $ S(U) = \{0, 1, 2\} $. We construct $ (X, Y) $ in the table at the left such that it strongly infers $ (S, T) $. The right table indicates $ x $ for each $ s $, $ \delta $ such that the definition of strong inference is satisfied: \\ 
 \begin {center}
 \begin{tabular}{ |c||c|c|c|c| } 

 \hline
 $ u $ & $ X(u) $ & $ Y(u) $ & $ S(u) $ & $ T(u) $ \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ 1 $ & $ 0 $ & $ 0 $ \\
 \hline
 $ 2 $ & $ 2 $ & $ -1 $ & $ 0 $ & $ 0 $ \\
 \hline
 $ 3 $ & $ 3 $ & $ 1 $ & $ 1 $ & $ 0 $ \\
 \hline
 $ 4 $ & $ 4 $ & $ -1 $ & $ 1 $ & $ 0 $ \\
 \hline 
 $ 5 $ & $ 5 $ & $ 1 $ & $ 2 $ & $ 1 $ \\
 \hline 
 $ 6 $ & $ 6 $ & $ -1 $ & $ 2 $ & $ 1 $ \\
 \hline
 \end{tabular} 
 \quad 
 \begin{tabular}{ |c||c|c| } 

 \hline
 $ s $ \textbackslash $ \delta $ & $ \delta_0 $ & $ \delta_1 $ \\ 
 \hline
 \hline
 $ 0 $ & $ 1 $ & $ 2 $  \\
 \hline
 $ 1 $ & $ 3 $ & $ 4 $ \\
 \hline
 $ 2 $ & $ 6 $ & $ 5 $ \\
 \hline
 
 \end{tabular}
 \end{center}
 
% EXAMPLE  
 \bigskip
 \bigskip 
 \textsc{Example.} \quad Let $ T(U) = \{1, 2, 3\} $ and $ S(U) = \{1, 2, 3, 4, 5\} $. Again, we construct $ (X, Y) $ in the table at the left such that it strongly infers $ (S, T) $. The right table indicates $ x $ for each $ s $, $ \delta $ such that the definition of strong inference is satisfied: \\ 

 \begin{center}
 \begin{tabular}{ |c||c|c|c|c| } 

 \hline
 $ u $ & $ X(u) $ & $ Y(u) $ & $ S(u) $ & $ T(u) $ \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ 1 $ & $ 1 $ & $ 1 $ \\
 \hline
 $ 2 $ & $ 2 $ & $ -1 $ & $ 1 $ & $ 1 $ \\
 \hline
 $ 3 $ & $ 3 $ & $ 1 $ & $ 2 $ & $ 1 $ \\
 \hline
 $ 4 $ & $ 4 $ & $ -1 $ & $ 2 $ & $ 1 $ \\
 \hline
 $ 5 $ & $ 5 $ & $ 1 $ & $ 3 $ & $ 2 $ \\
 \hline
 $ 6 $ & $ 6 $ & $ -1 $ & $ 3 $ & $ 2 $ \\
 \hline
 $ 7 $ & $ 7 $ & $ 1 $ & $ 4 $ & $ 2 $ \\
 \hline
 $ 8 $ & $ 8 $ & $ -1 $ & $ 4 $ & $ 2 $ \\
 \hline
 $ 9 $ & $ 9 $ & $ 1 $ & $ 5 $ & $ 3 $ \\
 \hline
 $ 10 $ & $ 10 $ & $ -1 $ & $ 5 $ & $ 3 $ \\
 \hline 
 \end{tabular} 
 \quad
 \begin{tabular}{ |c||c|c|c| } 

 \hline
 $ s $ \textbackslash $ \delta $ & $ \delta_1 $ & $ \delta_2 $ & $ \delta_3 $  \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ 2 $ & $ 2 $ \\
 \hline
 $ 2 $ & $ 3 $ & $ 4 $ & $ 4 $ \\
 \hline
 $ 3 $ & $ 6 $ & $ 5 $ & $ 6 $ \\
 \hline
 $ 4 $ & $ 8 $ & $ 7 $ & $ 8 $ \\
 \hline 
 $ 5 $ & $ 10 $ & $ 10 $ & $ 9 $ \\
 \hline
 
 \end{tabular}
 \end{center} 

% EXAMPLE
 \bigskip
 \bigskip 
 \textsc{Example.} \quad Let $ T(U) = \{1, 2, 3\} $ and $ S(U) = \{1, 2\} $. In this example, the inferred function $ f: S \rightarrow T, f(s) = T(S^{-1}(s)) $ is not single-valued. \\ 

 \begin{center}
 \begin{tabular}{ |c||c|c|c|c| } 

 \hline
 $ u $ & $ X(u) $ & $ Y(u) $ & $ S(u) $ & $ T(u) $ \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ -1 $ & $ 1 $ & $ 1 $ \\
 \hline
 $ 2 $ & $ 2 $ & $ -1 $ & $ 1 $ & $ 2 $ \\
 \hline
 $ 3 $ & $ 3 $ & $ 1 $ & $ 1 $ & $ 3 $ \\
 \hline
 $ 4 $ & $ 4 $ & $ -1 $ & $ 2 $ & $ 1 $ \\
 \hline
 $ 5 $ & $ 5 $ & $ -1 $ & $ 2 $ & $ 2 $ \\
 \hline
 \end{tabular} 
 \quad
 \begin{tabular}{ |c||c|c|c| } 

 \hline
 $ s $ \textbackslash $ \delta $ & $ \delta_1 $ & $ \delta_2 $ & $ \delta_3 $ \\ 
 \hline
 \hline
 $ 1 $ & $ 2 $ & $ 1 $ & $ 1 $  \\
  \hline
 $ 2 $ & $ 5 $ & $ 4 $ & $ 4 $  \\
 \hline
 
 \end{tabular}
 \end{center} 

\bigskip 
\bigskip
\newpage
% SECTION: INFERENCE OF TURING MACHINES
\section{Inference of Turing Machines}
% TURING INFERENCE THEOREM
\textsc{Theorem} \; \textit{A deterministic Turing machine} $(\Lambda, Q, \Delta)$ \textit{can be strongly inferred by a device iff} 
$$\forall s \in S(U),\; |S^{-1}(s)| \geq 2. $$ \textit{This holds for both the representation of a Turing machine as a partial recursive function and the representation as an update function.} \\

\textsc{Proof} \; First examine the partial function case. Let $ f $ be the partial recursive function that describes the given Turing machine tuple. Let $ U := \N $. Choose any convenient single valued surjective function $ S: U \rightarrow \B^{*} $ and define $ T: U \rightarrow \B^{*} \cup \eta$ by $T(u) = f(S(u)) $ as the single-valued function mapping $ U $ to the halting and non-halting outputs of $ f $. Then $ f $ can be written as the single-valued mapping $ S \rightarrow T $ by $ f(s) = T(S^{-1}(s)) $. \\

Enumerate the elements of $ S(U) $ as $ 1, 2, \dots, s, \dots \; $. Let $ V^s = \{u : S^{-1}(s) = u \} $ for $ s \in S(U) $. Similarly enumerate the elements of $ V^s $ as $ s_1, s_2, \dots , s_{|V^{s}|}$. Then define $ X $ and $ Y $ as follows:

\begin{equation*}
X(s_i) = \begin{cases}
       a_s & \text{ if } i = 1 \\
       b_s & \text{ otherwise } \\
       \end{cases} \quad \quad 
Y(s_i) = \begin{cases}
       1 & \text{ if } i = 1 \\
       -1 & \text{ otherwise } \\
       \end{cases} 
\end{equation*}

\bigskip
Note that the condition $ |S^{-1}(s)| \geq 2 $ is required to guarantee $ Y(V^s) = \{1, -1\} $. For each pair $ (s, \delta_{t \in T(U)} ) $, to force $ S(u) = s $ and $ Y(u) = \delta_{t}(T(u)) $, choose $ x = a_s $ if $ t = T(s_1) $ or otherwise choose $ x = b_s $. Since the choices of $ s $ and $ t $ were arbitrary, this holds for all $ (s, \delta_t) $ pairs. \\

Now consider the update function that describes the given Turing machine. Recall that the update function is written as $ \Delta: Q \times \Lambda^{k} \rightarrow Q \times \Lambda^{k - 1} \times \{ L, \mathcal{S}, R \} ^{k} $, $ k \geq 2 $. Consider a convenient single-valued surjective function $ S: U \rightarrow Q \times \Lambda^{k} $ representing the possible inputs for a Turing Machine and a corresponding single-valued $ T: U \rightarrow Q \times \Lambda^{k - 1} \times \{ L, \mathcal{S}, R \}^ {k} $ as $ T(u) = \Delta(S(u)) $. Observe that $ \Delta $ can be written as the single-valued function $ \Delta(s) = T(S^{-1}(s)) $. Then define $ V^s $, $ X $, $ Y $, and choose $ x $ for each $ (s, \delta_t ) $ as described in the preceding portion of the proof. Hence, the claim holds for the update function of a Turing machine.\\

To show that the condition is necessary for either representation, suppose that $|V^s| < 2$ for some $ s $. If $ V^s = \emptyset $ then there exists no $ x $ that can force $ S = s $. If $ |V| = 1 $, then we can assign $ Y(s_i) = y \in \{-1, 1\} $. However, whichever value is assigned, there exists a $ t $ such that $ \delta_{t}(T(s_i)) \neq Y(s_i) $ since $ |T(U)| \geq 2 $. $ \hfill \qed $ \\


\textsc{Remark.} Conventionally all functions over $ U $ must have a range of at least two elements. This implies that Turing machines that never halt cannot be strongly inferred.


%\bigskip
%\bigskip
%\textsc{Corollary} \quad \textit{}



%\bigskip
%\bigskip
%\textsc {Theorem} \quad \textit{Any non-deterministic Turing Machine can be strongly inferred by a device if} 
%$$\forall s \in S(U),\; |S^{-1}(s)| \geq 2. $$
%\textit{This condition holds for both the partial function representation and the instantaneous description of the Turing Machines.} \\

% # NOTES #
% Don't need to consider partial function case
% Need to construct two update functions 
%Consider the two update functions $ \Delta_0 $, $ \Delta_1 : Q \times \Lambda^{k} \rightarrow Q \times \Lambda^{k - 1} \times \{ L, S, R \} ^{k} $, $ k \geq 2 $ of a nondeterministic Turing machine. Let $ U := \N $. Choose any convenient single-valued surjective function $ S: U \rightarrow Q \times \Lambda^{k} $ and a corresponding single-valued $ T: U \rightarrow Q \times \Lambda^{k - 1} \times \{ L, S, R \}^ {k} $ as:
%
%\begin{equation*}
% T(u) = \begin{cases}
%        \Delta_0(S(u)) \\
%        \Delta_1(S(u)) \\
%        \end{cases} \text{arbitrarily}
%\end{equation*} 
%       
%Then we write $ \Delta_{0}(s), \Delta_{1}(s) = T(S^{-1}(s)) $


% Does this mean that there can be different t for each u? 

% #########

% SECTION: INFERENCE COMPLEXITY
\newpage
\section{Inference Complexity}

% INFERENCE COMPLEXITY DEFINITIONS
\textsc{Definition} \quad Let $ \mathcal{D} $ be an inference device and $ \Gamma $ be a function over $ U $ where $ X(U) $ and $ \Gamma(U) $ are countable and $ \mathcal{D} > \Gamma $. Let the \textbf{size} of $\gamma \in \Gamma(U) $ be written as $ \mathcal{M}_{\mu:\Gamma(\gamma)} = -\ln[\int_{\Gamma^{-1}(\gamma)} d\mu(u) 1] $ such that $ d\mu $ denotes a measure over $ U $. Then the \textbf{inference complexity} of $ \Gamma $ with respect to $ \mathcal{D} $ and measure $ \mu $ is defined as 

$$ \mathcal{C}_{\mu}(\Gamma ; \mathcal{D}) \triangleq \sum_{\delta \in \wp(\Gamma)} \min_{x : X = x \implies Y = \delta(\Gamma) } [\mathcal{M}_{\mu, X} (x)].	$$ 

The \textbf{strong inference complexity} for any single $ \gamma \in \Gamma $ is  

$$ \mathcal{C}_{\mu}(\gamma ; \mathcal{D}) \triangleq \min_{x : X(u) = x \implies S(u) = s, Y(u) = \delta_{\gamma}(\Gamma(u)) } [\mathcal{M}_{\mu, X} (x)].	$$ 


\bigskip
% MAXIMUM WEAK INFERENCE THEOREM
\textsc{Claim} \quad \textit{Let c be a positive integer. Let $ A^{*} $ be a countable set of inferable functions and $ \mathcal{D} $ be a device that infers all functions $ a \in A^{*} $. There exists $ A^{*} $ and $ \mathcal{D} $ such that every finite subset $ A $ of $ A^{*} $ with cardinality m has at least $ m(1 - m^{-c}) + 1 $ elements $ a \in A $ such that $ \mathcal{C}_{\mu}(a;\mathcal{D}) \geq \log m - c $ .} \\

\textsc{Proof} \quad Fix any convenient $ A^{*} $ and choose any subset $ A \subseteq A^{*} $ with cardinality $ m $. Let $ U := \B^{*} $. Let $ X: U \rightarrow \B^{*} $ and $ Y:U \rightarrow \{-1, 1\} $ be the $ X $ and $ Y $ defined in the Countable Inference Theorem when given $ A $. Note that $ X $ in this case maps $ \N \rightarrow \B^{*} $ instead of $ \N \rightarrow \N $ but the mappings are equivalent. Then $ \mathcal{D} = (X, Y) > A $ by construction.

\bigskip
Take $ d\mu(u) = \ell(u) $ where $ \ell(b) $ is the length of a bit string $ b $. Recall that the inference complexity of $ a \in A $ with respect to $ \mathcal{D} $ is
$$ \mathcal{C}_{\mu}(a ; \mathcal{D}) = \sum_{\delta \in \wp(a)} \min_{x : X = x \implies Y = \delta(a) } [\mathcal{M}_{\mu, X} (x)]: \mathcal{M}_{\mu, X} (x) = - \ln \sum_{X^{-1}(x)} {\ell(u)} $$

Now suppose that all $ A_i \in A $ is functionally equivalent to $ A_5 $ over $ V $. Then the maximum inference complexity of any $ A_i $ with respect to $ \mathcal{D} $ is
$$ \mathcal{C}_{\mu}(A_i ; \mathcal{D}) = \sum_{\delta \in \wp(A_i)} \min_{x : X = x \implies Y = \delta(a) } -\ln \sum_{X^{-1}(x)} {\ell(A_i^{-1}(a))} $$

$$ = \sum_{\delta \in \wp(A_i)} \min_{x : X = x \implies Y = \delta(a) } - \ln {A_i^{-1}(a)} = - \ln 1 - \ln(A_i^{-1}(a_i))  = - \ln(u). $$

Since a unique $ u $ is used by $ \mathcal{D} $ to infer every $ A_i \in A $, we can apply the pigeonhole principle. The number of bit strings of length less than $ m - c $ is 
$$ \sum_{i = 0}^{m - c - 1} {2^i} = 2^{m - c - 1}. $$

Hence, there are at least $ m - m2^{-c} + 1 $ functions $ a $ in $ A $ which have inference complexity greater than $ \ln \log m - c $.  \\

% INFERENCE COMPLEXITY LOWER BOUND THEOREM
\textsc{Theorem} \; \textit{ There exist $ A^{*} $ and $ \mathcal{D} $ such that for all functions of any subset $ A \subseteq A^{*} $, $ \mathcal{C}_{\mu}(A_i ; \mathcal{D}) = 0 $ } \\
\textsc{Proof} \quad 
Recall the functions and spaces defined in the Countable Inference Theorem and take $ \mu(u) = \ell (u) $. Then choose $ A^{*}: \overline{A_ i(V)} \in A^{*} = \overline{A_{j \in \{2, 3, 4\}}(V)} $. Then the inference complexity for any function $ A_i \in A $ is 
$$ \mathcal{C}_{\mu}(A_i ; \mathcal{D}) = \sum_{\delta \in \wp(A_i)} \min_{x : X = x \implies Y = \delta(A_i) } - 2 \ln \sum_{X^{-1}(x)} {\ell(u)} = \sum_{\delta \in \wp(f)} \min_{x : X = x \implies Y = \delta(A_i) } - \ln {1}  = - 2 \ln {1} = 0. $$
$ \hfill \qed $ 

\newpage 
% INCOMPRESSIBILITY
\section{Incompressibility}
% LI & VITANYI INCOMPRESSIBILITY THEOREM
\textit{The following result is taken from Li and Vitanyi (1993).} \\
\textsc{Incompressibility Theorem} \; \textit{Let c be a positive integer. For each fixed y, every finite set A of cardinality m has at least} $ m(1 - 2^{-c}) + 1 $ \textit{elements x with} $ \mathcal{C}(x|y) \geq \log m - c $. \\

\textsc{Proof} \; The number of programs of length less than $ \log m - c $ is 

$$ \sum_{i = 0}^{\log m - c - 1} {2^i} = 2^{\log m - c} - 1. $$

Hence, there are at least $ m - m2^{-c} + 1 $ elements in $ A $ which have no program of length less than $ \log m - c $. $ \hfill \qed $ \\

\textit{Note.} Let $ \mathcal{M}_{\mu:\Gamma(\gamma)} = [\int_{\Gamma^{-1}(\gamma)} d\mu(u) 1] $ for the remainder of this section. \\

\textsc{Theorem} \; \textit{There is a constant $ c $, such that for all $ t $:} 

$$
\mathcal{C}_{\mu}(t;\mathcal{D}) \leq 2\ell(t) + c
$$

\textsc{Proof} \; Let $ U := \B^{*} \times \{0, 1\} $. Fix a Turing machine $ \phi $ over $ U $ such that $ S: U \rightarrow \B^{*} $ is written as $ S((u, b)) = u $. Then define $ T: U \rightarrow \B^{*} $ as $ T(u) = S(u) $. Construct a device $ \mathcal{D} $ that strongly infers $ (S, T) $. Then for every $ t \in \B^{*} $, $ \mathcal{C}_{\mu}(t; \mathcal{D}) = 2\ell(t) + 2 $. \hfill \qed \\

% STRONG INFERENCE INCOMPRESSIBILITY RESULT

\textsc{Theorem} \; \textit{Let c be a positive integer. For a fixed $ \mathcal{D} $, every finite set $ T(U) $ of cardinality $ m $ has at least $ m(1 - 2^{-c - 1}) + \frac{1}{2} $ elements $ t $ with $ \mathcal{C}_{\mu}(t;\mathcal{D}) \geq \log m - c $. } \\

\textsc{Proof} \; Set $ U := \B^{*} $. Fix any universal partial recursive function $ \phi $ that can be strongly inferred. Let $ \mu(u) = \ell (u) $. Construct a device $ \mathcal{D} $ and functions $ (S, T) $ over $ \phi $ such that $ \mathcal{D} \gg (S, T) $. By the construction of $ \mathcal{D} $, it takes at least two unique values of $ u $ to force strong inference for each value $ t $. The number of bit strings of length less than $ \log m - c $ is 

$$ \sum_{i = 0}^{\log m - c - 1} {2^i} = 2^{\log m - c} - 1. $$

Then the greatest number of unique pairs of bit strings with combined length less or equal to $ \log m - c $ is $ m2^{- c - 1} - \frac{1}{2} $. Hence, there are at least $ m - m2^{-c - 1} + \frac{1}{2} $ values $ t $ with $ \mathcal{C}_{\mu}(t;\mathcal{D}) \geq \log m - c $. \hfill \qed \\ 
   
% INCOMPRESSIBILITY RESULT END

\textsc{Claim} \; \textit{(i) The function $ \mathcal{C}(\gamma;\mathcal{D}) $ is unbounded. (ii) Define a function $ m $ by $ m(x) = \min\{ \mathcal{C}(y;\mathcal{D}) : y \geq \gamma \}$. That is, $ m $ is the greatest monotonic increasing function bounding $ \mathcal{C} $ from below. The function $ m(x) $ is unbounded. (iii) For any partial recursive function $ \phi(x) $ which goes monotonic to infinity from some $ x_0 $ onward, we have $ m(x) < \phi(x) $ except for finitely many $ x $. In other words, although $ m(x) $ goes to infinity, it does so slower than any unbounded partial function. } \\

\textsc{Proof} \; (i) This follows immediately from (ii). (ii). For each $ i $ there is a least $ \gamma_{i} $ such that for all $ \gamma > \gamma_{i} $ the smallest set of $ u \in U $ strongly inferring $ \gamma $ has a total length greater or equal to $ i $. This follows immediately from the fact that there are only a finite number of $ u $ of each length $ i $. Clearly, for all $ i $ we have $ \gamma_{i + 1} \geq x_{i} $. Now observe that the function $ m $ has the property that $ m(\gamma) = i + 1 $ for $ \gamma_{i} < \gamma \leq \gamma_{i + 1} $. This proves (ii).


\newpage
\section{Physical Knowledge}
% Physical Knowledge

\textsc{Definition} \; Consider an inference device $ (X, Y) $ defined over $ U $, a function $ \Gamma $ defined over $ U $, a $ \gamma \in \Gamma(U) $, and a subset $ W \subseteq U $. We say that $ (X, Y) $ \textbf{(physically) knows} $ \Gamma = \gamma $ over $ W $ iff $\exists \xi : \Gamma(U) \rightarrow \overline{X} $ such that \\

\textit{i}) $ \forall \gamma' \in \Gamma(U), u \in \xi(\gamma') \implies \delta_{\gamma'}(\Gamma(u)) = Y(u) $ \\ 
\textit{ii}) $ \emptyset \neq \xi(\gamma) \cap W \subseteq Y^{-1}(1) $ \\
\textit{iii}) \textit{For all} $ \gamma' \neq \gamma $, $ \emptyset \neq \xi(\gamma') \cap W \subseteq Y^{-1}(-1) $. \\

A device $ \mathcal{D} $ physically knows a function $ \Gamma $ over $ W $ iff $ \forall \gamma \in \Gamma(U)$,  $ \exists \xi : \mathcal{D} $ knows $ \gamma $ over $ W \subseteq U $ using $ \xi $. \\


\textsc{Definition} \; The \textbf{knowledge complexity} of $ \Gamma $ given $ \mathcal{D} $, $\xi $, and measure $ \mu $ is defined as: 

$$ \mathcal{K}_{\mu}(\Gamma ; \mathcal{D}) \triangleq \sum_{\gamma \in \Gamma(U)} \min_{\xi(\gamma) : \mathcal{D} \text{ knows } \Gamma = \gamma \text{ using } \xi} [\mathcal{M}_{\mu; \xi} (\gamma)]	$$ 

$$ \mathcal{M}_{\mu; \xi} (\gamma) = -\ln \int_{\xi(\gamma)} d\mu(u) 1 $$ 

\textsc{Example.} \; Let $ U := \{1, 2, 3\} $ and take $ W = U $. The following table and formulas demonstrate physical knowledge for $ \gamma \in \Gamma : |\Gamma(U)| = 2 $. \\

\begin{tabular}{cc}
\begin{minipage}{0.45\textwidth}
    \begin{flushright}
         \begin{tabular}{ |c||c|c|c| } 
          \hline
          $ u $ & $ X(u) $ & $ Y(u) $ & $ \Gamma(u) $  \\ 
          \hline
          \hline
          $ 1 $ & $ 1 $ & $ 1 $ & $ 1 $ \\
          \hline
          $ 2 $ & $ 2 $ & $ -1 $ & $ 2 $ \\
          \hline
          $ 3 $ & $ 3 $ & $ 1 $ & $ 2 $ \\
          \hline
          $ 4 $ & $ 4 $ & $ -1 $ & $ 1 $ \\
          \hline
         \end{tabular}
   \end{flushright}
\end{minipage}&
\begin{minipage}{0.45\textwidth}

\begin{equation}
\xi_{1}(\gamma) = \begin{cases}
                      \{1\} & \text{ if } \gamma = 1 \\
                      \{4\} & \text{ if } \gamma = 2. \\
                \end{cases} \notag
\end{equation}
\begin{equation}
\xi_{2}(\gamma) = \begin{cases}
                      \{2\} & \text{ if } \gamma = 1 \\
                      \{3\} & \text{ if } \gamma = 2. \\
                \end{cases} \\ \notag
\end{equation} 

\end{minipage} 
\end{tabular}

\bigskip
\bigskip
\textsc{Lemma} \; \textit{For every countably-ranged function $ \Gamma $ there exists a device $ \mathcal{D} $ such that $ \mathcal{D} $ physically knows all $ \gamma \in \Gamma(U) $ over $ W = U $ if $ |\Gamma^{-1}(\gamma)| \geq 2 $ for all $ \gamma \in \Gamma $.} \\

\textsc{Proof} \; Take $ U := \N $ and $ W = U $. Choose any $ \Gamma $ as described in the claim. Enumerate $ \gamma \in \Gamma(U) $ as $ 1, 2, \dots, i, \dots $ . Define $ V^{i} = \{u : \Gamma^{-1}(\gamma) = u \} $ for each $ i \in \Gamma(U) $. Similarly enumerate each $ u \in V^{i} $ as $ i_1, i_2, \dots, i_j, \dots $ . Now define the device $ (X, Y) $ as 

\begin{equation*}
X(i_j) = \begin{cases}
       a_i & \text{ if } j = 1 \\
       b_i & \text{ if } j = 2 \\
       c_i & \text{ otherwise } \\
       \end{cases} \quad \quad 
Y(i_j) = \begin{cases}
       1 & \text{ if } j = 1 \\
       -1 & \text{ otherwise. } \\
       \end{cases} 
\end{equation*}

Define $ \xi_{\gamma} : \Gamma(U) \rightarrow \overline{X} $ for each $ i \in \Gamma(U) $  as

\begin{equation*}
\xi_{\gamma}(i) = \begin{cases}
       \overline{a_i} & \text{ if } i = \gamma \\
       \overline{b_i} & \text{ otherwise. } \\
       \end{cases} 
\end{equation*}

Then for all $ \gamma \in \Gamma(U) $ and all $ u \in \xi_{\gamma}(\gamma')$, $ \delta_{\gamma}(\Gamma(u)) = Y(u) $. Furthermore, for each $ \xi_{\gamma} $, $ \emptyset \neq \{ ( \xi_{\gamma}(\gamma' = \gamma) \cap W) \subseteq Y^{-1}(1) \} $ and $ \emptyset \neq \{ ( \xi_{\gamma}(\gamma' \neq \gamma) \cap  W ) \subseteq Y^{-1}(-1) \} $. Hence, $ \mathcal{D} $ physically knows $ \gamma \in \Gamma(U) $. \hfill \qed \\

\textsc{Claim} \; \textit{For any set of independently physically knowable characteristic functions $ A^{*} $ there is a device $ \mathcal{D} $ and set of functions $ \xi{*} $ such that $ \mathcal{D} $ knows $ A \in A^{*} $ over a given subset $ W \subseteq U $ using $ \xi^{*} $. } \\

\textsc{Proof} \;  

%%%%%%%%%%%%%%%%%%%%%%%% 
Let $ U := \N $ and take $ W = U $. Construct 

\begin{equation*}
X(i_j) = \begin{cases}
       a_i & \text{ if } j = 1 \\
       b_i & \text{ otherwise } \\
       \end{cases} \quad \quad 
Y(i_j) = \begin{cases}
       1 & \text{ if } j = 1 \\
       -1 & \text{ otherwise. } \\
       \end{cases} 
\end{equation*}

%%%%%%%%%%%%%%%%%%%%%%%%

\bigskip
\textsc{Claim} \; \textit{There exists a countable set of inequivalent characteristic functions $ A^{*} $ and a device $ \mathcal{D} $ such that for all $ A \in A^{*} $, $ \mathcal{D} $ physically knows $ A $ over $ U $ and $ \mathcal{K}_{\mu}(A;\mathcal{D}) = 0 $. } \\

\textsc{Proof} \; 

\bigskip
\textsc{Claim} \; \textit{Let $ \Gamma $ be a knowable function with countable range and $ \mathcal{D} $ be a device that physically knows all $ \gamma \in \Gamma $ over $ U $. Every such $ \Gamma $ with $ |\Gamma(U)| = m $ has knowledge complexity of at least $ - m \ln \sum_{i = 1}^{m} {\log_{2}{m}} $.} \\ 

\textsc{Proof} \;
Let $ U := \N $ and take $ W = U $. Choose any convenient knowable $ \Gamma $ with a countable range such that $ |\Gamma(U)| = m $. Then construct $ \mathcal{D} $ and $ \xi_{ \gamma \in \Gamma(U) } $ as specified in the knowability lemma. Then the knowledge complexity of $ \Gamma $ with respect to $ \mathcal{D} $ using $ \xi_{\gamma \in \Gamma(U)} $ is

$$ \mathcal{K}_{\mu}(\Gamma ; \mathcal{D}) = \sum_{\gamma \in \Gamma(U)} \min_{\xi(\gamma) : \mathcal{D} \text{ knows } \Gamma = \gamma \text{ using } \xi} [\mathcal{M}_{\mu; \xi} (\gamma)]	$$

$$ = \sum_{\gamma \in \Gamma(U)}  \min_{\xi(\gamma) : \mathcal{D} \text{ knows } \Gamma = \gamma \text{ using } \xi} - \ln \sum_{u \in \xi_{\gamma}(\gamma \in \Gamma(U))} \ell(u) = - m \ln \sum_{i = 1}^{m} {\log_{2}{m}} $$ 


% SECTION: DISCUSSION
\section{Discussion} 


% SECTION: OPEN QUESTIONS
\section{Open Questions}

Much of the results covered in this manuscript are preliminary and leaves much to be desired in our understanding of the theory of inference devices and its connections to algorithmic information theory. There are many immediate questions deserving attention. The question of whether non-deterministic Turing machines may be strongly inferred was left unattended. Our incompressibility results with strong inference of universal Turing machines may be extended by exploring possible analogues between Kolmogorov complexity and strong inference complexity. The numerous negative results we found concerning the definitions of weak inference complexity and physical knowledge suggest that a revisitation of these definitions may be appropriate or fruitful.

% SECTION: ACKNOWLEDGEMENTS
\section{Acknowledgements}
I am grateful to David Wolpert for his mentorship and for many crucial discussions in the process of this work. I would like to thank the education office of the Santa Fe Institute (SFI) and the participants of the SFI Research Experience for Undergraduates (REU) program for providing resources and assistance. This work was supported by the National Science Foundation, Grant No. 1757923. \\

\bigskip
\noindent\rule{\textwidth}{1pt} \\

\begin{enumerate}[label={[\arabic*]}]

\item Wolpert DH. \textit{Constraints on physical reality arising from a formalization of knowledge}. arXiv preprint arXiv:1711.03499v3 [physics.hist-ph] (2018). 
\item Li M \& Vitányi P. \textit{An Introduction to Kolmogorov Complexity and Its Applications} (1st ed.). Springer-Verlag. (1993). 
\item Arora S \& Barak B. \textit{Computational Complexity: A Modern Approach}. Cambridge University Press. (2009). 

\end{enumerate}

\end{document}

