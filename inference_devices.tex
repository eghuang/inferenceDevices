\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}          
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{csquotes}
\usepackage{setspace}\onehalfspacing
\usepackage[loose,nice]{units} %replace "nice" by "ugly" for units in upright fractions
\usepackage{graphicx}
\graphicspath{ {/Users/eghuang/math104/images/} }
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=R,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\title{
  Notes on Inference Devices \\
  \large Santa Fe Institute}
\author{Edward G. Huang}
\date{Summer 2018} 
 
 \newcommand{\R}{\mathbb{R}}
 \newcommand{\Q}{\mathbb{Q}}
 \newcommand{\Z}{\mathbb{Z}}
 \newcommand{\N}{\mathbb{N}}
 \newcommand{\B}{\mathbb{B}}
 \newcommand{\Prob}{\mathbb{P}}
 \newcommand{\E}{\mathbb{E}}
 \newcommand{\code}[1]{\texttt{#1}}
 \let\oldemptyset\emptyset
 \let\emptyset\varnothing
 
 \setlength{\parindent}{0pt} % no indent
 
\begin{document}
\maketitle 

% HOOPER: Please include an abstract, a brief intro with a clear statement of your research question(s), a description of your methods/model/approach, your results, and your conclusions (or lessons learned).

% SECTION 0: ABSTRACT
\section{Abstract} 

 

% SECTION 1: NOTATION
\section{Introduction} 


\subsection{Notation and Definitions}
This manuscript utilizes standard notation taken from set theory and vector algebra. We clarify notation specific to Turing machine theory and inference devices. \\

\textbf{Turing Machine Notation} \\
$ \B^{*} \quad $ The space of all finite bit strings. \\
$ \Lambda \quad $ Symbol alphabet of a Turing Machine. \\
$ \sigma \quad $ A symbol on a Turing Machine tape. \\
$ Q \quad $ Set of finite states of a Turing Machine. \\
$ \Delta \quad $ Transition function of a Turing Machine. \\
$ k \quad $ Number of tapes of a Turing Machine. The first tape is assumed to be read-only. \\
$ \eta \quad $ Non-halting state of a Turing Machine. \\

\textbf{Inference Device Notation} \\
$ U \quad $ Set of possible histories of the universe. \\
$ u \quad $ A history of the universe in $ U $. \\ 
$ X \quad $ Setup function of an ID that maps $ U \rightarrow X(U) $. A binary question concerning $ \Gamma(u) $. \\
$ x \quad $ A binary question and a member of image $ X(U) $. \\ 
$ Y \quad $ Single-valued conclusion function of an ID that maps $ U \rightarrow \{-1, 1\} $. A binary answer of an ID for  $ X(u) = x $. \\ 
$ y \quad $ A single-valued answer, and member of image $ Y(U)  = \{0, 1\} $. \\ 
$ \Gamma \quad $ A function of the actual values of a physical variable over $U$, equivalent to $\Gamma(u) = S(t_i)(u)$.  \\
$ \gamma \quad $ Possible value of a physical variable, a member of the image $\Gamma(U)$. \\
$ \delta \quad $ Probe of any variable $V$ parameterized by $v \in V$ such that : 
	  \[ \delta_v (v') =
	  \begin{cases} 
       1 & \text{ if } v = v' \\
       -1 & \text{ otherwise } \\
      \end{cases}\] \\
$ \wp \quad $ Set of probes over $\Gamma(U)$. \\
$ \mathcal{D} = (X, Y) \quad $ An inference device, consisting of functions $ X $ and $ Y $. \\
$ \quad \bar{F} \quad $ Inverse. Given a function $ F $ over $ U $, $F ^ {-1} = \bar{F} \equiv \{\{u : F(u) = f \} : f \in F(U) \} $. \\
$ > \quad $ Weak inference: a device $\mathcal{D}$ weakly infers $\Gamma$ \textit{iff} $ \forall \gamma \in \Gamma(U), \exists x \in X(U) $ s.t. $ \forall u \in U $, 

$ \quad \quad X(u) = x \implies Y(u) = \delta_{\gamma}(\Gamma(u)) $.  \\
$ \gg \quad $ Strong inference: a device $ (X, Y) $ strongly infers a functions $ (S, T) $ over $ U $ \textit{ iff } $\forall \delta \in \wp(T) $ 

\quad \quad and all $ s \in S(U) $, $ \exists x $ \textit{ such that } $ X(u) = x \implies S(u) = s, Y(u) = \delta(T(u)) $. \\

% SECTION 2: TURING MACHINES
\subsection{Turing Machines} 

\textbf{Deterministic Turing Machines} \\
 Arora and Barak denote a Turing Machine (TM) as $ T = (\Lambda, Q, \Delta) $ containing:
\begin{enumerate}
\item An \textit{alphabet} $ \Lambda $ of a finite set of symbols that $ T $'s tapes can contain. We assume that $ \Lambda $ contains a special blank symbol $ B $, start symbol $ S $, and the symbols 0 and 1. 
\item A finite set $ Q $ of possible states that $ T $'s register can be in. We assume that $ Q $ contains a special start state $ q_{s} $ and a special halt state $ q_{h} $. 
\item A transition function $ \Delta : Q \times \Lambda^{k} \rightarrow Q \times \Lambda^{k - 1} \times \{L, \mathcal{S}, R\}^{k} $, where $ k \geq 2$, describing the rules $ T $ use in performing each step. The set $\{L, \mathcal{S}, R\}$ denote the actions \textit{Left, Stay,} and \textit{Right}, respectively. 
\end{enumerate}

Suppose $ T $ is in state $ q \in Q $ and $ (\sigma_1, \sigma_2, \dots, \sigma_k) $ are the symbols on the $ k $ tapes. Then $ \Delta(q, (\sigma_1, \dots, \sigma_k)) = (q', (\sigma_{2}^{'}, \dots, \sigma_{k}^{'}), z) $ where $ z \in \{L, \mathcal{S}, R\}^k $ and at the next step the $ \sigma $ symbols in the last $ k - 1 $ tapes will be replaced by the $ \sigma' $ symbols, the machine will be in state $ q $, and the $ k $ heads will move \textit{Left, Right} or \textit{Stay}. This is illustrated in Figure 2.1. \\

\textbf{Figure 2.2.1. The transition function $ \Delta $ for a $ k $-tape Turing Machine}

 \begin{center}
 \begin{tabular}{ c|c|c|c||c|c|c|c|c } 
 \hline
 \multicolumn{4}{c||}{ $ (q, (\sigma_1, \dots, \sigma_k)) $ } & 
      \multicolumn{5}{c}{ $ (q', (\sigma_{2}^{'}, \dots, \sigma_{k}^{'}), z) $ } \\
 
 \hline
 \begin{tabular}[c]{@{}c@{}} Input \\ symbol \end{tabular} & 
 \begin{tabular}[c]{@{}c@{}} Work/output \\ symbol \\ read \end{tabular} & 
 $\dots $ &
 \begin{tabular}[c]{@{}c@{}} Current \\ state \end{tabular} &
 \begin{tabular}[c]{@{}c@{}} New \\ work/output \\ tape symbol \end{tabular} & 
 $ \dots $ &
 \begin{tabular}[c]{@{}c@{}} Move \\ work/output \\ tape \end{tabular} &
 $ \dots $ &
 \begin{tabular}[c]{@{}c@{}} New \\ state \end{tabular} \\ 
 
 \hline
 \hline
 $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ \\ 
 \hline
 $  \sigma_1 $ & $ \sigma_i $ & $ \ddots $ & $ q $ & $ \sigma_i^{'} $ & $ \ddots $ & $ z_i $ & $ \ddots $ & $ q^{'} $ \\ 
 \hline
 $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ & $ \ddots $ & $ \vdots $ \\ 
 \end{tabular}
 \end{center} 

\bigbreak

\textit{Remark}: $\Lambda$ can be reduced to $ \B = \{0, 1\} $ and $ k $ can be reduced to $ 1 $ without loss of computational power. Then, any Turing Machine can be expressed as a partial recursive function mapping $ \B^{*} \rightarrow \B^{*} \cup \eta $, where $ \eta $ is the undefined non-halting output. Since $ |\B^{*} \times \B^{*} \cup \eta | = | \N \times \N | = | \N | $, the set of all Turing Machines is countably infinite. \\

\textbf{Non-deterministic Turing Machines} \\
Non-deterministic Turing Machines (NDTM) differ from deterministic Turing Machines by having two transition functions $ \Delta_0, \Delta_1 $ and a special state $ q_{accept} $. From Arora and Barak:

\begin{displayquote}
When a NDTM $M$ computes a function, we envision that at each computational step $ M $ makes an arbitrary choice as to which of its two transition functions to apply. For every input $ x $, we say that $ M(x) = 1 $ if there exists some sequence of these choices (which we call nondeterministic choices of $ M $) that would make $ M $ reach $ q_{accept} $ on input $ x $. Otherwise - if every sequence of choices makes $ M $ halt without reaching $ q_{accept} $ - then we say that $ M(x) = 0 $. 
\end{displayquote}

If $ M(x) = 1$, we say that $ M $ accepts the input $ x $. There are two ways to interpret the choice of update function to use in a NDTM. We can either assume that the NDTM chooses updates that will lead to an accepting state, or we can assume that the machine branches out into its choices such that it has a "computation tree" and if any of the branches reaches the accepting state then the machine accepts the input. From this second interpretation, the computational power of DTMs to NDTMs is analogous to the computational complexity of P to NP. \\

% SECTION 3: STRONG INFERENCE
\section{Strong Inference} 
 
In the next three examples we examine strong inference of integer-valued functions. \\
 
 
 \textbf{Example 2.3.1} \quad Let $ T(U) = \{0 , 1\} $ and $ S(U) = \{0, 1, 2\} $. We construct $ (X, Y) $ in the table at the left such that it strongly infers $ (S, T) $. The right table indicates $ x $ for each $ s $, $ \delta $ such that the definition of strong inference is satisfied: \\ 
 \begin {center}
 \begin{tabular}{ |c||c|c|c|c| } 

 \hline
 $ u $ & $ X(u) $ & $ Y(u) $ & $ S(u) $ & $ T(u) $ \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ 1 $ & $ 0 $ & $ 0 $ \\
 \hline
 $ 2 $ & $ 2 $ & $ -1 $ & $ 0 $ & $ 0 $ \\
 \hline
 $ 3 $ & $ 3 $ & $ 1 $ & $ 1 $ & $ 0 $ \\
 \hline
 $ 4 $ & $ 4 $ & $ -1 $ & $ 1 $ & $ 0 $ \\
 \hline 
 $ 5 $ & $ 5 $ & $ 1 $ & $ 2 $ & $ 1 $ \\
 \hline 
 $ 6 $ & $ 6 $ & $ -1 $ & $ 2 $ & $ 1 $ \\
 \hline
 \end{tabular} 
 \quad 
 \begin{tabular}{ |c||c|c| } 

 \hline
 $ s $ \textbackslash $ \delta $ & $ \delta_0 $ & $ \delta_1 $ \\ 
 \hline
 \hline
 $ 0 $ & $ 1 $ & $ 2 $  \\
 \hline
 $ 1 $ & $ 3 $ & $ 4 $ \\
 \hline
 $ 2 $ & $ 6 $ & $ 5 $ \\
 \hline
 
 \end{tabular}
 \end{center}
 
  
 \bigskip
 \bigskip 
 \textbf{Example 2.3.2} \quad Let $ T(U) = \{1, 2, 3\} $ and $ S(U) = \{1, 2, 3, 4, 5\} $. Again, we construct $ (X, Y) $ in the table at the left such that it strongly infers $ (S, T) $. The right table indicates $ x $ for each $ s $, $ \delta $ such that the definition of strong inference is satisfied: \\ 

 \begin{center}
 \begin{tabular}{ |c||c|c|c|c| } 

 \hline
 $ u $ & $ X(u) $ & $ Y(u) $ & $ S(u) $ & $ T(u) $ \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ 1 $ & $ 1 $ & $ 1 $ \\
 \hline
 $ 2 $ & $ 2 $ & $ -1 $ & $ 1 $ & $ 1 $ \\
 \hline
 $ 3 $ & $ 3 $ & $ 1 $ & $ 2 $ & $ 1 $ \\
 \hline
 $ 4 $ & $ 4 $ & $ -1 $ & $ 2 $ & $ 1 $ \\
 \hline
 $ 5 $ & $ 5 $ & $ 1 $ & $ 3 $ & $ 2 $ \\
 \hline
 $ 6 $ & $ 6 $ & $ -1 $ & $ 3 $ & $ 2 $ \\
 \hline
 $ 7 $ & $ 7 $ & $ 1 $ & $ 4 $ & $ 2 $ \\
 \hline
 $ 8 $ & $ 8 $ & $ -1 $ & $ 4 $ & $ 2 $ \\
 \hline
 $ 9 $ & $ 9 $ & $ 1 $ & $ 5 $ & $ 3 $ \\
 \hline
 $ 10 $ & $ 10 $ & $ -1 $ & $ 5 $ & $ 3 $ \\
 \hline 
 \end{tabular} 
 \quad
 \begin{tabular}{ |c||c|c|c| } 

 \hline
 $ s $ \textbackslash $ \delta $ & $ \delta_1 $ & $ \delta_2 $ & $ \delta_3 $  \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ 2 $ & $ 2 $ \\
 \hline
 $ 2 $ & $ 3 $ & $ 4 $ & $ 4 $ \\
 \hline
 $ 3 $ & $ 6 $ & $ 5 $ & $ 6 $ \\
 \hline
 $ 4 $ & $ 8 $ & $ 7 $ & $ 8 $ \\
 \hline 
 $ 5 $ & $ 10 $ & $ 10 $ & $ 9 $ \\
 \hline
 
 \end{tabular}
 \end{center} 

 \bigskip
 \bigskip 
 \textbf{Example 2.3.3} \quad Let $ T(U) = \{1, 2, 3\} $ and $ S(U) = \{1, 2\} $. In this example, the inferred function $ f: S \rightarrow T, f(s) = T(S^{-1}(s)) $ is not single-valued. \\ 

 \begin{center}
 \begin{tabular}{ |c||c|c|c|c| } 

 \hline
 $ u $ & $ X(u) $ & $ Y(u) $ & $ S(u) $ & $ T(u) $ \\ 
 \hline
 \hline
 $ 1 $ & $ 1 $ & $ -1 $ & $ 1 $ & $ 1 $ \\
 \hline
 $ 2 $ & $ 2 $ & $ -1 $ & $ 1 $ & $ 2 $ \\
 \hline
 $ 3 $ & $ 3 $ & $ 1 $ & $ 1 $ & $ 3 $ \\
 \hline
 $ 4 $ & $ 4 $ & $ -1 $ & $ 2 $ & $ 1 $ \\
 \hline
 $ 5 $ & $ 5 $ & $ -1 $ & $ 2 $ & $ 2 $ \\
 \hline
 \end{tabular} 
 \quad
 \begin{tabular}{ |c||c|c|c| } 

 \hline
 $ s $ \textbackslash $ \delta $ & $ \delta_1 $ & $ \delta_2 $ & $ \delta_3 $ \\ 
 \hline
 \hline
 $ 1 $ & $ 2 $ & $ 1 $ & $ 1 $  \\
  \hline
 $ 2 $ & $ 5 $ & $ 4 $ & $ 4 $  \\
 \hline
 
 \end{tabular}
 \end{center} 

\bigskip 
\bigskip
% SECTION 4: INFERENCE OF TURING MACHINES
\section{Inference of Turing Machines}
\textbf{Theorem} \quad \textit{A deterministic Turing machine} $(\Lambda, Q, \Delta)$ \textit{can be strongly inferred by a device iff} 
$$\forall s \in S(U),\; |S^{-1}(s)| \geq 2. $$ \textit{This holds for both the representation of a Turing Machine as a partial recursive function and the representation as an update function.} \\

\textbf{Proof} \quad First examine the partial function case. Let $ f $ be the partial recursive function that describes the given Turing machine tuple. Let $ U := \N $. Choose any convenient single valued surjective function $ S: U \rightarrow \B^{*} $ and define $ T: U \rightarrow \B^{*} \cup \eta$ by $T(u) = f(S(u)) $ as the single-valued function mapping $ U $ to the halting and non-halting outputs of $ f $. Then $ f $ can be written as the single-valued mapping $ S \rightarrow T $ by $ f(s) = T(S^{-1}(s)) $. \\

% delta and f must be in definitions of s and t.

Enumerate the elements of $ S(U) $ as $ 1, 2, \dots, s, \dots \; $. Let $ V^s = \{u : S^{-1}(s) = u \} $ for $ s \in S(U) $. Similarly enumerate the elements of $ V^s $ as $ s_1, s_2, \dots , s_{|V^{s}|}$. Then define $ X $ and $ Y $ as follows:

\begin{equation*}
X(s_i) = \begin{cases}
       a_s & \text{ if } i = 1 \\
       b_s & \text{ otherwise } \\
       \end{cases} \quad \quad 
Y(s_i) = \begin{cases}
       1 & \text{ if } i = 1 \\
       -1 & \text{ otherwise } \\
       \end{cases} 
\end{equation*}

\bigskip
Note that the condition $ |S^{-1}(s)| \geq 2 $ is required to guarantee $ Y(V^s) = \{1, -1\} $. For each pair $ (s, \delta_{t \in T(U)} ) $, to force $ S(u) = s $ and $ Y(u) = \delta_{t}(T(u)) $, choose $ x = a_s $ if $ t = T(s_1) $ or otherwise choose $ x = b_s $. Since the choices of $ s $ and $ t $ were arbitrary, this holds for all $ (s, \delta_t) $ pairs. \\

Now consider the update function that describes the given Turing machine. Recall that the update function is written as $ \Delta: Q \times \Lambda^{k} \rightarrow Q \times \Lambda^{k - 1} \times \{ L, \mathcal{S}, R \} ^{k} $, $ k \geq 2 $. Consider a convenient single-valued surjective function $ S: U \rightarrow Q \times \Lambda^{k} $ representing the possible inputs for a Turing Machine and a corresponding single-valued $ T: U \rightarrow Q \times \Lambda^{k - 1} \times \{ L, \mathcal{S}, R \}^ {k} $ as $ T(u) = \Delta(S(u)) $. Observe that $ \Delta $ can be written as the single-valued function $ \Delta(s) = T(S^{-1}(s)) $. Then define $ V^s $, $ X $, $ Y $, and choose $ x $ for each $ (s, \delta_t ) $ as described in the preceding portion of the proof. Hence, the claim holds for the update function of a Turing machine.\\

To show that the condition is necessary for either representation, suppose that $|V^s| < 2$ for some $ s $. If $ V^s = \emptyset $ then there exists no $ x $ that can force $ S = s $. If $ |V| = 1 $, then we can assign $ Y(s_i) = y \in \{-1, 1\} $. However, whichever value is assigned, there exists a $ t $ such that $ \delta_{t}(T(s_i)) \neq Y(s_i) $ since $ |T(U)| \geq 2 $. $ \hfill \qed $ \\


\textit{Remark:} Conventionally all functions over $ U $ must have a range of at least two elements. This implies that Turing machines that never halt cannot be strongly inferred.


%\bigskip
%\bigskip
%\textbf{Corollary} \quad \textit{}



%\bigskip
%\bigskip
%\textbf {Theorem} \quad \textit{Any non-deterministic Turing Machine can be strongly inferred by a device if} 
%$$\forall s \in S(U),\; |S^{-1}(s)| \geq 2. $$
%\textit{This condition holds for both the partial function representation and the instantaneous description of the Turing Machines.} \\

% # NOTES #
% Don't need to consider partial function case
% Need to construct two update functions 
%Consider the two update functions $ \Delta_0 $, $ \Delta_1 : Q \times \Lambda^{k} \rightarrow Q \times \Lambda^{k - 1} \times \{ L, S, R \} ^{k} $, $ k \geq 2 $ of a nondeterministic Turing machine. Let $ U := \N $. Choose any convenient single-valued surjective function $ S: U \rightarrow Q \times \Lambda^{k} $ and a corresponding single-valued $ T: U \rightarrow Q \times \Lambda^{k - 1} \times \{ L, S, R \}^ {k} $ as:
%
%\begin{equation*}
% T(u) = \begin{cases}
%        \Delta_0(S(u)) \\
%        \Delta_1(S(u)) \\
%        \end{cases} \text{arbitrarily}
%\end{equation*} 
%       
%Then we write $ \Delta_{0}(s), \Delta_{1}(s) = T(S^{-1}(s)) $


% Does this mean that there can be different t for each u? 

% #########

% SECTION: INFERENCE COMPLEXITY
\section{Inference Complexity}

\textbf{Definition} \quad Let $ \mathcal{D} $ be an inference device and $ \Gamma $ be a function over $ U $ where $ X(U) $ and $ \Gamma(U) $ are countable and $ \mathcal{D} > \Gamma $. Let the \textbf{size} of $\gamma \in \Gamma(U) $ be written as $ \mathcal{M}_{\mu:\Gamma(\gamma)} = -\ln[\int_{\Gamma^{-1}(\gamma)} d\mu(u) 1] $ such that $ d\mu $ denotes a measure over $ U $. Then the \textbf{inference complexity} of $ \Gamma $ with respect to $ \mathcal{D} $ and measure $ \mu $ is defined as: 

$$ \mathcal{C}_{\mu}(\Gamma ; \mathcal{D}) \triangleq \sum_{\delta \in \wp(\Gamma)} min_{x : X = x \implies Y = \delta(\Gamma) } [\mathcal{M}_{\mu, X} (x)]	$$ 

% INCOMPRESSIBILITY THEOREM
\textbf{Incompressibility Theorem} \quad \textit{Let c be a positive integer. For each fixed y, every finite set A of cardinality m has at least} $ m(1 - m^{-c}) + 1 $ \textit{elements x with} $ \mathcal{C}(x|y) \geq \log m - c $. \\

\textbf{Proof} \quad The number of programs of length less than $ \log m - c $ is 

$$ \sum_{i = 0}^{\log m - c - 1} {2^i} = 2^{\log m - c} - 1. $$

Hence, there are at least $ m - m2^{-c} + 1 $ elements in $ A $ which have no program of length less than $ \log m - c $. $ \hfill \qed $ \\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bigskip
\bigskip

\textbf{Lemma 5.1} \; \textit{A function} $ \Gamma $ \textit{can be weakly inferred by a device} $ \mathcal{D} $ \textit{if} $ |\Gamma^{-1}(\gamma)| \geq 2 $ \textit{for any} $ \gamma \in \Gamma(U) $. \\
\textbf{Proof} \; Let $ U := \N $. Enumerate $ \gamma \in \Gamma $ as $ 1, 2, \dots, i, \dots, n $ and define $ V^{i} = \{u: \Gamma^{-1}(i) = u \} $. Then enumerate each element in $ V^i $ as $ i_1, i_2, \dots, i_j, \dots, i_m $. Continue to define $ X $ and $ Y $ as 
\begin{equation*}
X(i_j) = \begin{cases}
       a_i & \text{ if } j = 2 \\
       b_i & \text{ otherwise } \\
       \end{cases} \quad \quad 
Y(i_j) = \begin{cases}
       -1 & \text{ if } j = 2 \\
       1 & \text{ otherwise. } \\
       \end{cases} 
\end{equation*}

Then for each $ i \in \Gamma(U) $ choose $ x = b_i $ to force $ Y(X^{-1}(b_i)) = 1 = \delta_{i}(\Gamma(X^{-1}(b_i)) = i). \hfill \qed $ \\

\bigskip
\textbf{Proposition} \quad \textit{No inference device that can weakly infer all sets of finite-ranged functions.} \\
\textbf{Proof} \quad Take any conclusion function $ Y $. Let $\Gamma(u) = Y(u) $. Then for $ \gamma = -1 $, $ X(u) = x \implies Y(u) = -1 \neq \delta_{-1}(\Gamma(u) = -1) \hfill \qed $  \\


\bigskip

% You can define a device to infer a given set of countably infinite functions.

% Perhaps the best approach is to find a way to define a device give any finite set of functions and prove any of these devices behave the same under inference complexity

\textbf{Corollary} \quad \textit{A set of countable functions A can be weakly inferred by a device} $ \mathcal{D} $ \textit {if for all} $ F \in A $,  $\exists f \in F : |F^{-1}(f)| \geq 2 $. \\

\textbf{Proof} \quad We proceed by induction. Let the base case be $ |A| = 1 $. The base case holds by Theorem 5.1. Assume that the claim holds for some $ n $ such that $ |A| = n $. We show that it holds for the case of $ n + 1 $. Let $ U := \N $ and denote the device inferring $ A $ as $ (X, Y) $. Add a new function $ a $ to $ A $ such that $\exists g \in a(U) : |a^{-1}(g)| \geq 2 $ and $ a $ is not already a member of $ A $. Enumerate $ g \in a $ as $ 1, 2, \dots, i, \dots, n $ and define $ V^{i} = \{u: a^{-1}(i) = u \} $. Then enumerate each element in $ V^i $ as $ i_1, i_2, \dots, i_j, \dots, i_m $.

Then define the inference device $ ( X', Y' ) $ as 

\begin{equation*}
X'(i_j) = \begin{cases}
       b_i & \text{ if } j = 2 \\
       X(i_j) & \text{ otherwise } \\
       \end{cases} \quad \quad 
Y'(i_j) = \begin{cases}
       -1 & \text{ if } j = 2 \\
       Y(i_j) & \text{ otherwise } \\
       \end{cases} \\ 
\end{equation*}

Then for each $ i \in \Gamma(U) $ choose $ x = a_i $ to force $ Y(X^{-1}(a_i)) = 1 = \delta_{i}(\Gamma(X^{-1}(a_i)) = i) $. Because the choice of $ n $ was arbitrary, the claim holds for all $ n \in \N. \hfill \qed $ \\

\bigskip
\textbf{Inference Incompressibility Theorem} \quad \textit{Let c be a positive integer. Let $ A^{*} $ be the set of all functions with finite ranges. Every finite subset $ A $ of $ A^{*} $ with cardinality m has at least} $ m(1 - m^{-c}) + 1 $ \textit{elements a such that} $ \mathcal{C}_{\mu}(a;\mathcal{D}) \geq \log m - c $. \\


\textbf{Proof} \quad We first construct a fixed device $ \mathcal{D}_{\phi} $ that can weakly infer any finite set $ A $. Let $ U := \B^{*} \cup \{0\} $. Define $ X: U \rightarrow \B^{*} \cup \{0\} $ as $ X(u) = u $ and define $ :Y $


\bigskip
Let $ U := \B^{*} $ and take $ d\mu(u) = \ell(u) $ where $ \ell(b) $ is the length of a bit string $ b $. Choose any finite set $ A(U) $ with cardinality $ m $. Since $ A(U) $ is finite, its elements are bit strings. 

$$ \mathcal{C}_{\mu}(A ; \mathcal{D}) \triangleq \sum_{\delta \in \wp(A)} min_{x : X = x \implies Y = \delta(\Gamma) } [\mathcal{M}_{\mu, X} (x)]	$$ 

$$ \mathcal{M}_{\mu, X} (x) = - \ln \sum_{X^{-1}(x)} {\ell(u)} $$


The number of $ a $ with length of less than $ \log m - c $ is less than the sum

$$ \sum_{i = 0}^{\log m - c} {2^i} = ^{\log m - c} - 1. $$ 

Note that each $ u $ can infer at most one unique element of $ A $. Hence, there are at least $ m - m2^{-c} + 1 $ elements in $ A $ that cannot be inferred by a $ u $ of length less than $ \log m - c $. \\

%$ \hfill \qed $ \\



%\section{Future Work}
%\section{Acknowledgements}

\end{document}